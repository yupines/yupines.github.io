[{"content":"I am a M.S. student in Shanghai jiaotong University. And my major is immunology. In Mid-2020, I transferred to dedicated bioinformatic analysis. My researches focus on profiling multi-omics data to decipher heterogenous immune microenvironment. Besides, I'm intrested in machine learning in augmenting omics-data integration and their extensive applications in immune biology. Here, I wanna blog my learning experience and share with everyone！🙋 \nLatest\nuniPort: a unified computational framework for single-cell data integration with optimal transport https://www.biorxiv.org/content/10.1101/2022.02.14.480323v1.article-metrics  \n","permalink":"https://yupines.github.io/me/","summary":"I am a M.S. student in Shanghai jiaotong University. And my major is immunology. In Mid-2020, I transferred to dedicated bioinformatic analysis. My researches focus on profiling multi-omics data to decipher heterogenous immune microenvironment. Besides, I'm intrested in machine learning in augmenting omics-data integration and their extensive applications in immune biology. Here, I wanna blog my learning experience and share with everyone！🙋 \nLatest\nuniPort: a unified computational framework for single-cell data integration with optimal transport https://www.","title":"About me"},{"content":" 经过一段时间东拼西凑的学习，小白终于能理解变分自编码器了哈哈哈，感谢各路大神的教程！在这里整理一些简短的笔记。\n一、VAE的初步理解  1. 给定真实样本$X_k$,假设存在专属于$X_k$的后验分布$p(Z|X_k)$,并假定这个分布是正态多元的，专属的目的是训练生成器$X = g(Z)$,从分布$p(Z|X_k)$采样出一个$Z_k$并还原为$\\hat{X_k}$。\n2. 使用神经网络拟合正态分布的参数：均值$\\mu$和方差${\\sigma}^2$。于是构建两个神经网络：${\\mu}k = {f}_1(X_k)$和${\\log}{\\sigma}_k^2 = {f}_2(X_k)$ （不需要激活函数的处理）。\n3. 重构X的过程需最小化$(\\hat{X_k},X_k)$的距离，重构噪声（即方差）既要尽可能地小（减少重构误差），同时又要防止退化为0（保证模型具有生成能力），那么可以让所有样本的$p(Z|X_k)$向标准正态分布看齐。若接近正态分布$N(0,I)$，则： $$p(Z) = \\sum\\limits_{X}p(Z|X)p(X) = \\sum N(0,I)p(X) = N(0,I)$$ $p(Z)$满足正态分布。接下来利用KL散度来计算loss，即$KL(N(\\mu,{\\sigma}^2)||N(0,I))$，计算结果为：\n$$\\mathcal{L}_{\\mu,{\\sigma}^2} = \\frac{1}{2}\\sum\\limits_{i=1}^d({\\mu}_{(i)}^2 + {\\sigma}^2_{(i)}-{\\log}{\\sigma}^2_{(i)}-1) $$ 其中$d$是因变量$Z$的维度。\n 4. 重参数技巧：从正态分布中采样 $\\varepsilon$，然后$Z = \\mu + \\varepsilon * \\sigma$  二、VAE的目标函数  （一）背景知识  1.边缘分布  假设$p(x,y)$,其中一个特定变量的边缘分布为： $$p(x) = \\sum\\limits_{y}p(x,y) = \\sum\\limits_{y}p(x|y)p(y)$$  2.交叉熵和KL散度  交叉熵和KL关系密切，可以用来衡量两个分布的差异，以离散变量$x$为例： $$H(P,Q)=-\\mathbb{E}_{x\\sim P}\\ {\\log}Q(x) = -\\sum\\limits_{i=1}^N P(x_i){\\log}Q(x_i)$$ $KL$散度定义，对$P,Q$分布有： $$D_{KL}(P||Q) = \\mathbb{E}_{x\\sim P}[{\\log}\\frac{P(x)}{Q(x)}] = \\mathbb{E}_{x\\sim P}[{\\log}P(x) - {\\log}Q(x)] $$ 对于离散型变量有： $$D_{KL}(P||Q) = \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}(\\frac{P(x_i)}{Q(x_i)}) = \\sum\\limits_{i=1}^N P(x_i)\\ [{\\log}P(x_i)-{\\log}Q(x_i)] $$ 对于连续型变量，其概率密度函数（probability density function，PDF）用小写字母$p(x)$和$q(x)$来表示，离散型随机变量的概率质量函数（probability mass function，PMF）用大写字母表示。\n以离散变量为例，对于$KL$散度公式展开可得到： $$\\begin{aligned} D_{KL}(P||Q) \u0026= \\sum\\limits_{i=1}^N P(x_i)\\ [{\\log}P(x_i)-{\\log}Q(x_i)] \\\\ \u0026= \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}P(x_i) - \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}Q(x_i) \\\\ \u0026= -[-\\sum\\limits_{i=1}^N P(x_i)\\ {\\log}P(x_i)]+[-\\sum\\limits_{i=1}^N P(x_i)\\ {\\log}Q(x_i)] \\\\ \u0026=-H(P) + H(P,Q) \\end{aligned}$$ 即$H(P,Q) = H(P) + D_{KL}(P||Q)$  3.EM算法  Expectation Maximization Algorithm,一种迭代算法，用于含有隐变量的概率参数模型的最大似然估计或极大后验概率估计。分为Expection-Step 和 Maximization-Step。E-Step 主要通过观察数据和现有模型来估计参数，由此计算似然函数的期望值；M-Step 寻找似然函数最大时对应的参数。\nEM算法的一般思路： $$\\begin{aligned} L(\\theta) \u0026= \\mathbb{E}_{x\\sim data}[{\\log}\\ p_{\\theta}(x)] \\\\ \u0026= \\mathbb{E}_{x\\sim data}[{\\log}\\ p_{\\theta}(z,x)-{\\log}\\ p_{\\theta}(z|x)]\\\\ \u0026= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z,x)]-\\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z|x)] \\\\ \u0026= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z,x)] + H(\\phi) + KL(\\phi||\\theta) \\end{aligned}$$ 引入任意分布$z$和参数$\\theta,\\forall \\phi$ STEPS：初始化参数$\\theta$，重复计算E和M步骤\nE-Step：根据$p_{\\theta}(z)$和$p_{\\theta}(x|z)$求出reference，即$p_{\\theta}(x|z)$，令${\\phi}={\\theta}$; M-Step：固定$\\phi$后，寻找新的参数$\\theta$，使得$\\mathbb{E}_{x\\sim data}[{\\log}_{\\theta}(x)]$增大（即‘辅助函数’）。 在迭代中，$\\mathbb{E}_{x\\sim data}[{\\log}\\ p_{\\theta}(x)]$不断增大，$H(\\phi)$与$\\theta$无关，$KL(\\phi||\\theta) = 0$ 变成了 $KL(\\phi||\\theta)0$，于是随着迭代进行，目标函数$L(\\theta)$不断增大。  （二）从概率图模型的角度理解   VAE的训练近似EM  假设一个概率图模型，可以生成隐变量$z$，然后生成观测变量$x$，参数为$\\theta$。得到分布：$p_{\\theta}(z)$和$p_{\\theta}(x|z)$。由此推测隐藏分布：$x$的边缘分布$p_{\\theta}(x)$和$z$的条件分布$p_{\\theta}(z|x)$。模型可以采用最大似然法训练，找到使得$L(\\theta) = \\mathbb{E}_{x\\sim data}{\\log}\\ p_{\\theta}(x)$最大的参数$\\theta$。 对于VAE来说，$p_{\\theta}(z)$ 固定为$N(0,I)$，而$p_{\\theta}(x|z)$ 由decoder定义，于是EM算法中的目标函数变成： $$\\begin{aligned} L(\\theta) \u0026= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z,x)] + H(\\phi) + KL(\\phi||\\theta) \\\\ \u0026= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z)\\ +{\\log}\\ p_{\\theta}(x|z)] + H(\\phi) + KL(\\phi||\\theta) \\\\ \u0026= \\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -[H(\\phi)+KL(\\phi||p_{\\theta}(z))]+ H(\\phi) + KL(\\phi||\\theta) \\\\ \u0026= \\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -KL(\\phi||p_{\\theta}(z))+ KL(\\phi||\\theta) \\end{aligned}$$ 此时，理想的EM算法步骤为：\nE-Step：令$\\phi$为概率图模型中隐含的条件分布$p_{\\theta}(z|x)$；\nS-Step：寻找新的$\\theta$使得$L(\\theta)$继续增大。\n但对于VAE而言，$p_{\\theta}(z|x)$无法显示表达出来，于是VAE设计一个参数为$\\phi$的网络来近似$\\theta$。实际EM算法如下：\nE-Step：令$\\phi$逼近 $\\theta$；\nM-Step：寻找 $\\theta$ 来增大 $\\mathbb{E}_{\\phi}{\\log}$ 可以看出迭代的过程主要使 $\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -KL(\\phi||p_{\\theta}(z))$ 增大，使 $KL(\\phi||\\theta)$ 逼近零。\n于是定义：Evidence Lower BOund（ELBO） $$ELBO(\\theta,\\phi) = \\mathbb{E}_{x\\sim data}[\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -\\ KL({\\phi}||N(0,I))] $$ evidence指的是目标函数 ${\\log}\\ p_{\\theta}(x)$，因为 $KL(\\phi||\\theta)$ 非负，所以前两项为目标函数的下界。ELBO即为VAE的目标函数，由于E-M步增大的目标是相同的，可以合并，用梯度上升法来最大化ELBO，最大化目标函数的下界，以此间接最大化目标函数。 （三）从神经网络的角度理解  VAE是带有正则化的自编码器  普通自编码器由编码器 $z = g_{\\phi}(x)$ 和解码器 $x = f_{\\theta}(z)$ 构成。目标函数是最小化 $D(x,f_{\\theta}(g_{\\phi}(x)))$。若$x$是二值，则 $f_{\\theta}(z)$ 代表伯努利分布均值；若$x$是任意实数，则 $f_{\\theta}(z)$ 代表正态分布，重构误差可以写成 $-{\\log}\\ p_{\\theta}(x|z)$，最大似然训练等价于最小化重构误差: $$L_{reconstruct}(\\theta,\\phi) = -\\ \\mathbb{E}_{x\\sim data}\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z) $$ VAE除了重构，还要考虑生成和插值的问题，需要让编码器的输出$z$分布服从$N(0,I)$，于是添加正则项： $$L_{reg}(\\theta,\\phi) = KL(\\mathbb{E}_{x\\sim data}\\ p_{\\phi}(z|x)||N(0,I)) $$ 因有 $KL(\\mathbb{E}_p||q)\\leq \\mathbb{E}KL(p||q)$，则VAE的总损失函数为：\n$$\\begin{aligned} L(\\theta,\\phi) \u0026=L_{reconstruct}(\\theta,\\phi)+L_{reg}(\\theta,\\phi)\\\\ \u0026= -\\ \\mathbb{E}_{x\\sim data}\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z) + KL(\\mathbb{E}_{x\\sim data}\\ p_{\\phi}(z|x)||N(0,I)) \\\\ \u0026\\leq \\mathbb{E}_{x\\sim data}[- \\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z) + KL(p_{\\phi}(z|x)||N(0,I))] \\end{aligned}$$ 即得到ELBO！(o゜▽゜)o☆[BINGO!]  参考链接：\nhttps://kexue.fm/archives/5253 https://zhuanlan.zhihu.com/p/366204765\nhttps://zhuanlan.zhihu.com/p/78311644\n ","permalink":"https://yupines.github.io/post/vae/","summary":"经过一段时间东拼西凑的学习，小白终于能理解变分自编码器了哈哈哈，感谢各路大神的教程！在这里整理一些简短的笔记。\n一、VAE的初步理解  1. 给定真实样本$X_k$,假设存在专属于$X_k$的后验分布$p(Z|X_k)$,并假定这个分布是正态多元的，专属的目的是训练生成器$X = g(Z)$,从分布$p(Z|X_k)$采样出一个$Z_k$并还原为$\\hat{X_k}$。\n2. 使用神经网络拟合正态分布的参数：均值$\\mu$和方差${\\sigma}^2$。于是构建两个神经网络：${\\mu}k = {f}_1(X_k)$和${\\log}{\\sigma}_k^2 = {f}_2(X_k)$ （不需要激活函数的处理）。\n3. 重构X的过程需最小化$(\\hat{X_k},X_k)$的距离，重构噪声（即方差）既要尽可能地小（减少重构误差），同时又要防止退化为0（保证模型具有生成能力），那么可以让所有样本的$p(Z|X_k)$向标准正态分布看齐。若接近正态分布$N(0,I)$，则： $$p(Z) = \\sum\\limits_{X}p(Z|X)p(X) = \\sum N(0,I)p(X) = N(0,I)$$ $p(Z)$满足正态分布。接下来利用KL散度来计算loss，即$KL(N(\\mu,{\\sigma}^2)||N(0,I))$，计算结果为：\n$$\\mathcal{L}_{\\mu,{\\sigma}^2} = \\frac{1}{2}\\sum\\limits_{i=1}^d({\\mu}_{(i)}^2 + {\\sigma}^2_{(i)}-{\\log}{\\sigma}^2_{(i)}-1) $$ 其中$d$是因变量$Z$的维度。\n 4. 重参数技巧：从正态分布中采样 $\\varepsilon$，然后$Z = \\mu + \\varepsilon * \\sigma$  二、VAE的目标函数  （一）背景知识  1.边缘分布  假设$p(x,y)$,其中一个特定变量的边缘分布为： $$p(x) = \\sum\\limits_{y}p(x,y) = \\sum\\limits_{y}p(x|y)p(y)$$  2.交叉熵和KL散度  交叉熵和KL关系密切，可以用来衡量两个分布的差异，以离散变量$x$为例： $$H(P,Q)=-\\mathbb{E}_{x\\sim P}\\ {\\log}Q(x) = -\\sum\\limits_{i=1}^N P(x_i){\\log}Q(x_i)$$ $KL$散度定义，对$P,Q$分布有： $$D_{KL}(P||Q) = \\mathbb{E}_{x\\sim P}[{\\log}\\frac{P(x)}{Q(x)}] = \\mathbb{E}_{x\\sim P}[{\\log}P(x) - {\\log}Q(x)] $$ 对于离散型变量有： $$D_{KL}(P||Q) = \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}(\\frac{P(x_i)}{Q(x_i)}) = \\sum\\limits_{i=1}^N P(x_i)\\ [{\\log}P(x_i)-{\\log}Q(x_i)] $$ 对于连续型变量，其概率密度函数（probability density function，PDF）用小写字母$p(x)$和$q(x)$来表示，离散型随机变量的概率质量函数（probability mass function，PMF）用大写字母表示。","title":"Variational Auto-Encoder"}]