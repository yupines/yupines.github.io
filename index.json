[{"content":"I am a M.S. student in Shanghai Institute of Immunology, Shanghai Jiao Tong University School of Medicine. In mid-2020, I transferred from molecular experiments to bioinformatics analysis. Generally my researches focus on profiling single-cell multi-omics data to decipher heterogenous immune microenvironment. I'm particularly interested in applying machine learning to develope new computational algorithms for the integration of large- scale genomic data, analysis of molecular dynamics and construction of immune networks. Here, I want to blog my learning experience and share with everyone！ \nLatest\nuniPort: a unified computational framework for single-cell data integration with optimal transport. Kai Cao*, Qiyu Gong*, Yiguang Hong, Lin Wan. https://www.biorxiv.org/content/10.1101/2022.02.14.480323v1 \n","permalink":"https://yupines.github.io/me/","summary":"I am a M.S. student in Shanghai Institute of Immunology, Shanghai Jiao Tong University School of Medicine. In mid-2020, I transferred from molecular experiments to bioinformatics analysis. Generally my researches focus on profiling single-cell multi-omics data to decipher heterogenous immune microenvironment. I'm particularly interested in applying machine learning to develope new computational algorithms for the integration of large- scale genomic data, analysis of molecular dynamics and construction of immune networks. Here, I want to blog my learning experience and share with everyone！","title":"About me"},{"content":" Manuscripts \nuniPort: a unified computational framework for single-cell data integration with optimal transport. Kai Cao*, Qiyu Gong*, Yiguang Hong, Lin Wan. https://www.biorxiv.org/content/10.1101/2022.02.14.480323v1 \nMeta-transcriptomic analysis reveals the gene expression and novel conserved sub-genomic RNAs in SARS-CoV-2 and MERS-CoV. Lin Lyu, Ru Feng, Mingnan Zhang, Qiyu Gong, Yinjing Liao, Yanjiao Zhou, Xiaokui Guo, Bing Su, Yair Dorsett. https://www.biorxiv.org/content/10.1101/2020.04.16.043224v2 \n In advance \nMechanism of FGL1 regulation of liver cancer tissue-resident memory CD103+CD69+CD8+T cell function. Single-cell profiling the crosstalk mechanism between Schwann and fibroblast in neurofibromatosis type one. ","permalink":"https://yupines.github.io/paper/","summary":"Manuscripts \nuniPort: a unified computational framework for single-cell data integration with optimal transport. Kai Cao*, Qiyu Gong*, Yiguang Hong, Lin Wan. https://www.biorxiv.org/content/10.1101/2022.02.14.480323v1 \nMeta-transcriptomic analysis reveals the gene expression and novel conserved sub-genomic RNAs in SARS-CoV-2 and MERS-CoV. Lin Lyu, Ru Feng, Mingnan Zhang, Qiyu Gong, Yinjing Liao, Yanjiao Zhou, Xiaokui Guo, Bing Su, Yair Dorsett. https://www.biorxiv.org/content/10.1101/2020.04.16.043224v2 \n In advance \nMechanism of FGL1 regulation of liver cancer tissue-resident memory CD103+CD69+CD8+T cell function.","title":"Publication"},{"content":"\n\u0026ensp; \u0026ensp;Single cell \u0026ensp; \u0026ensp;Multi-omics \u0026ensp; \u0026ensp;Immunology \u0026ensp; \u0026ensp;Pan-cancer \u0026ensp; \u0026ensp;Data integration ","permalink":"https://yupines.github.io/tags/","summary":"\n\u0026ensp; \u0026ensp;Single cell \u0026ensp; \u0026ensp;Multi-omics \u0026ensp; \u0026ensp;Immunology \u0026ensp; \u0026ensp;Pan-cancer \u0026ensp; \u0026ensp;Data integration ","title":"Tag"},{"content":"“流形”是指局部与欧式空间同胚的空间，在局部建立降维映射关系后再投影到全局.\n\n 1.等度量映射（Isometric mapping, Isomap）  \n在局部空间内，基于欧式距离构建近邻连接图，在近邻图上获得任意两点间的距离后，采用MDS方法获得样本在地位空间的坐标。\n背景知识：\n多维尺度变换 (multidimensional scaling,MDS）\n给定\\(n\\)个对象之间的相似性或距离，在低维空间表示这些对象时，最大程度得保留对象间的接近程度。依据度量标准是否为欧式距离时，分为经典MDS和非经典MDS。\nClassical MDS 给定\\(n\\)维空间中的距离矩阵\\(D (N\\times N)\\)，第\\(i\\)行\\(j\\)列表示对象\\(i,j\\)之间的距离，假设数据降维到\\(Z\\)维空间（假设中心化，因此行列和为0），要求对象在\\(Z\\)维空间中的距离与原始距离相近，于是有 \\[d_{ij}^2=||z_i-z_j||=z_i^2+z_j^2-2z_i^Tz_j \\] 依次求和得到 \\[\\sum\\limits_{i=1}^n d_{ij}^2 = \\sum\\limits_{i=1}^n z_{i}^2 + N z_j^2 \\] \\[\\sum\\limits_{j=1}^n d_{ij}^2 = \\sum\\limits_{j=1}^n z_{j}^2 + N z_i^2 \\] \\[\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n d_{ij}^2 = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n z_j^2 + N\\sum\\limits_{i=1}^n z_i^2 = 2N\\sum\\limits_{i=1}^n z_i^2 \\] 定义内积矩阵\\(B=Z^TZ\\)，整合上述等式得到 \\[ b_{ij} = -\\frac{1}{2}(\\frac{1}{N^2}\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n d_{ij}^2-\\frac{1}{N}\\sum\\limits_{i=1}^nd_{ij^2}-\\frac{1}{N}\\sum\\limits_{j=1}^nd_{ij^2}+d_{ij}^2) \\] 因为\\(B\\)是对称矩阵，有\\(B=V \\Lambda V^T\\)，降维之后的数据点坐标为\\(Z=\\Lambda^{1/2} V_* \\in R^{d_*\\times n}\\). ISOmap Steps: 输入：样本集 \\(D=\\{d_1,d_2,\\ldots,d_m\\}\\)\n近邻参数\\(k\\)\n低维空间维数\\(d\u0026#39;\\)\n步骤：for \\(i\\) in \\(1,2,\\ldots,m\\) do 确定\\(x_i\\)的\\(k\\)近邻，将其距离设置为欧氏距离，其他距离设置为无穷大；\nend for 调用最短路径计算任意两点间的欧式距离\\(d_{ij}\\)并输入MDS\n输出：样本集\\(D\\)在低维空间中的投影\\(Z\\) 以上方法是针对训练样本的投影，针对新数据，可以用训练样本训练一个回归学习器，来对新样本的低维空间坐标做预测。 针对近邻图构建有两种方法：一是指定\\(k\\)，二是指定距离阈值，小于此值的被认为是近邻。 \n 2.局部线性嵌入（Locally linear embedding,LLE）  \n区别于ISOmap保持局部邻近样本的距离，LLE意在保持局部样本的线性关系。\n假设样本\\(x_i\\)能通过邻近样本\\(x_j,x_k,x_l\\)重构出来，则有 \\[x_i = w_{ij}x_j+w_{ik}x_k+w_{il}x_l \\] LLE先为样本 \\(x_i,i=(1,2,\\ldots,m)\\)，找到近邻下标集合\\(Q_i\\)，然后计算出重构系数集合\\(w_i\\) \\[ \\mathop{min}\\limits_{w_1,w_2,\\ldots,w_m} \\sum\\limits_{i=1}^m||x_i-\\sum\\limits_{j\\in Q_i} w_{ij}x_j||^2 \\\\ s.t. \\sum\\limits_{j\\in Q_i} w_{ij} =1 \\] \\(x_i,x_j\\)均已知，对目标函数进行变形 \\[ \\sum\\limits_{i=1}^m||\\sum\\limits_{j\\in Q_i} w_{ij}x_i-\\sum\\limits_{j\\in Q_i} w_{ij}x_j||^2= \\sum\\limits_{i=1}^m ||X_iw_i||^2 = \\sum\\limits_{i=1}^m w_i^TX_i^TX_iw_i \\] 其中，\\(w_i = (w_{iq_i^1},w_{iq_i^2},\\ldots,w_{iq_i^n}) \\in R^{n\\times 1}\\)，\\(X_i= (,x_i-x_{q_i^2},\\ldots,x_i-x_{q_i^n}) \\in R^{d \\times n}\\)。同理，约束条件可变形为 \\[ \\sum\\limits_{j\\in Q_i} w_{ij} =w_i^TI=1 \\] 其中，\\(I=(1,1,\\ldots,1) \\in R^{n \\times 1}\\).优化问题可以改写为 \\[ \\mathop{min}\\limits_{w_1,w_2,\\ldots,w_m} \\sum\\limits_{i=1}^m w_i^TX_i^TX_iw_i \\\\ s.t.\\quad w_i^TI=1 \\] 转化为带约束的优化问题，使用拉格朗日乘子法求解 \\[ L(w_1,w_2,\\ldots,w_m,\\lambda) = \\sum\\limits_{i=1}^m w_i^TX_i^TX_iw_i + \\lambda(w_i^TI-1) \\] 对\\(w_i\\)求偏导数并令其等于0 \\[\\begin{aligned} \\frac{L(w_1,w_2,\\ldots,w_m,\\lambda)}{\\partial w_i} \u0026amp;= \\frac{\\partial[\\ \\sum\\limits_{i=1}^m w_i^TX_i^TX_iw_i + \\lambda(w_i^TI-1)\\ ]}{\\partial w_i} =0 \\\\ \u0026amp;= \\frac{\\partial[\\ w_i^TX_i^TX_iw_i + \\lambda(w_i^TI-1)\\ ]}{\\partial w_i} = 0 \\end{aligned}\\] 由微分矩阵公式得 \\(\\frac{\\partial x^TBx}{\\partial x} = (B+B^T),\\frac{\\partial x^Ta}{\\partial x}=a\\) 可得 \\[ \\frac{\\partial[\\ w_i^TX_i^TX_iw_i + \\lambda(w_i^TI-1)\\ ]}{\\partial w_i} = 2X_i^TX_iw_i+\\lambda I =0 \\\\ X_i^TX_iw_i = -\\frac{1}{2}\\lambda I \\] 若\\(X_i^TX_i\\)可逆（\\(X_i\\)的列向量线性无关），则 \\[ w_i = -\\frac{1}{2}\\lambda (X_i^TX_i)^{-1}I \\] 因为 \\(w_i^TI=I^Tw_i=1\\)，上式同时左乘\\(I^T\\)得到 \\[ I^Tw_i = -\\frac{1}{2}\\lambda I^T(X_i^TX_i)^{-1}I \\\\ -\\frac{1}{2}\\lambda = \\frac{1}{I^T(X_i^TX_i)^{-1}I} \\] 将结果代回可得到 \\[ w_i = \\frac{(X_i^TX_i)^{-1}I}{I^T(X_i^TX_i)^{-1}I} \\] 令 \\((X_i^TX_i)^{-1}\\) 第\\(j\\)行和\\(k\\)列的元素为\\(C^{-1}_{jk}\\)，则 \\[ w_{ij} = w_{iq_j^i} = \\frac{\\sum\\limits_{k\\ \\in Q_i} C^{-1}_{jk}}{\\sum\\limits_{l,s\\ \\in Q_i} C^{-1}_{ls}} \\] LLE在低维空间中保持线性重构系数不变，于是\\(x_i\\)对应的低维空间坐标满足 \\[ \\mathop{min}\\limits_{w_1,w_2,\\ldots,w_m} \\sum\\limits_{i=1}^m||z_i-\\sum\\limits_{j\\in Q_i} w_{ij}z_j||^2 \\]\n令 \\(Z=(z_1,z_2,\\ldots,z_m)\\in R^{d\u0026#39;\\times m},(W)_{ij}=w_{ij},M=(I-W)^T(I-W)\\)，则 \\[\\begin{aligned} \\mathop{min}\\limits_{Z} \\sum\\limits_{i=1}^m||z_i-\\sum\\limits_{j\\in Q_i} w_{ij}z_j||^2 \u0026amp;= \\sum\\limits_{i=1}^m||ZI_i- ZW_i||^2 \\\\ \u0026amp;= \\sum\\limits_{i=1}^m (I_i-W_i)^TZ^TZ(I_i-W_i) \\\\ \u0026amp;= tr((I-W)^TZ^TZ(I-W)) \\\\ \u0026amp;= tr(Z(I-W)(I-W)^TZ^T) \\\\ \u0026amp;= tr(ZMZ^T) \\end{aligned}\\] 其中，\\(M=(I-W)(I-W)^T\\)，则优化目标可以改写成 \\[ \\mathop{min}\\limits_{Z}\\ tr(ZMZ^T) \\\\ s.t.\\ ZZ^T=I \\] 约束条件是为了得到标准正交的低维空间。 LLE Steps: 输入：样本集 \\(D=\\{d_1,d_2,\\ldots,d_m\\}\\)\n近邻参数\\(k\\)\n低维空间维数\\(d\u0026#39;\\)\n步骤：for \\(i\\) in \\(1,2,\\ldots,m\\) do 确定\\(x_i\\)的\\(k\\)近邻集合\\(Q_i\\)，求得\\(w_{ij},j\\in Q_i\\)，对于\\(j\\notin Q_i, w_{ij} = 0\\)；\nend for 计算\\(M\\)，并进行特征值分解\nreturn \\(M\\)最小\\(d\u0026#39;\\)个特征值对应的特征向量构成集合\\(Z^T\\) 输出：样本集\\(D\\)在低维空间中的投影\\(Z\\) \n参考链接：\n知乎-机器学习算法-MDS降维算法 \n","permalink":"https://yupines.github.io/post/manifold-learning/","summary":"“流形”是指局部与欧式空间同胚的空间，在局部建立降维映射关系后再投影到全局.\n\n 1.等度量映射（Isometric mapping, Isomap）  \n在局部空间内，基于欧式距离构建近邻连接图，在近邻图上获得任意两点间的距离后，采用MDS方法获得样本在地位空间的坐标。\n背景知识：\n多维尺度变换 (multidimensional scaling,MDS）\n给定\\(n\\)个对象之间的相似性或距离，在低维空间表示这些对象时，最大程度得保留对象间的接近程度。依据度量标准是否为欧式距离时，分为经典MDS和非经典MDS。\nClassical MDS 给定\\(n\\)维空间中的距离矩阵\\(D (N\\times N)\\)，第\\(i\\)行\\(j\\)列表示对象\\(i,j\\)之间的距离，假设数据降维到\\(Z\\)维空间（假设中心化，因此行列和为0），要求对象在\\(Z\\)维空间中的距离与原始距离相近，于是有 \\[d_{ij}^2=||z_i-z_j||=z_i^2+z_j^2-2z_i^Tz_j \\] 依次求和得到 \\[\\sum\\limits_{i=1}^n d_{ij}^2 = \\sum\\limits_{i=1}^n z_{i}^2 + N z_j^2 \\] \\[\\sum\\limits_{j=1}^n d_{ij}^2 = \\sum\\limits_{j=1}^n z_{j}^2 + N z_i^2 \\] \\[\\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^n d_{ij}^2 = \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n z_j^2 + N\\sum\\limits_{i=1}^n z_i^2 = 2N\\sum\\limits_{i=1}^n z_i^2 \\] 定义内积矩阵\\(B=Z^TZ\\)，整合上述等式得到 \\[ b_{ij} = -\\frac{1}{2}(\\frac{1}{N^2}\\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n d_{ij}^2-\\frac{1}{N}\\sum\\limits_{i=1}^nd_{ij^2}-\\frac{1}{N}\\sum\\limits_{j=1}^nd_{ij^2}+d_{ij}^2) \\] 因为\\(B\\)是对称矩阵，有\\(B=V \\Lambda V^T\\)，降维之后的数据点坐标为\\(Z=\\Lambda^{1/2} V_* \\in R^{d_*\\times n}\\). ISOmap Steps: 输入：样本集 \\(D=\\{d_1,d_2,\\ldots,d_m\\}\\)","title":"Manifold Learning"},{"content":"在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。\n\n （一）.背景知识  \n 1.马氏距离（Mahalanobis Distance）  \n度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个\\(d\\)维样本\\(x_i,x_j\\)之间的马氏距离为\n\\[ dist_{mah}(x_i,x_j) = \\sqrt{(x_i - x_j)^T\\Sigma^{-1}(x_i - x_j)} \\] \\(\\Sigma\\) 为协方差矩阵.\n从欧式距离出发，\\(d\\)维样本\\(x_i,x_j\\)之间的欧式距离为 \\[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 = dist_{i,j,1}^2 + dist_{i,j,2}^2 + \\ldots + dist_{i,j,d}^2 \\] 对每一维\\(d\\)引入权重\\(w\\)得到 \\[\\begin{aligned} dist_{wed}^2(x_i,y_i) \u0026amp;= w_1·dist_{i,j,1}^2 + w_2·dist_{i,j,2}^2 + \\ldots + w_d·dist_{i,j,d}^2 \\\\ \u0026amp;= (x_i-x_j)^TM(x_i-x_j) \\end{aligned}\\] 其中\\(w \\geq 0,W=diag(w)，W\\)是一个对角矩阵.\n将\\(W\\)换成半正定对称矩阵\\(M\\)，即得到马氏距离. \\[ dist_{Mah} = ||x_i-x_j||^2_M \\] \\(M\\)即是度量矩阵，为使度量矩阵非负，\\(M\\)必须是（半）正定对称矩阵，满足\\(M=PP^T\\).\n马氏距离的几何意义:\n将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. \n 2.近邻分类器（neighbourhood Component Analysis, NCA）  \n对\\(M\\)的学习需要设定目标. 以近邻分类器为例，假设希望提高NCA的性能，则可将\\(M\\)嵌入到NCA的评价指标.\n计算样本\\(i\\)的近邻分布 \\[ p_{ij} = \\frac{exp(-||x_i-x_j||^2_M)}{\\sum\\limits_{k\\neq i}exp(-||x_i-x_k||^2_M)} \\] \\(x_j\\)对\\(x_i\\)的影响随着距离的增大而减少. 以留一法(LOO)正确率最大化为目标，计算\\(x_i\\)的留一法正确率 \\[ p_i = \\sum\\limits_{j \\in \\Omega_i} p_{ij} \\] \\(\\Omega_i\\)表示与\\(x_i\\)同类别的样本的下标集合，整个样本集的留一法正确率为 \\[\\sum\\limits_{i=1}^m p_i=\\sum\\limits_{i=1}^m \\sum\\limits_{j \\in \\Omega_i} p_{ij} \\] 将\\(p_{ij}\\)及\\(M=PP^T\\)代入上式可得NCA的优化目标为 \\[ \\mathop{min}\\limits_{P}\\quad 1- \\sum\\limits_{i=1}^m \\sum\\limits_{j \\in \\Omega_i} \\frac{exp(-||P^T x_i-P^Tx_j||^2)}{\\sum\\limits_{k\\neq i}exp(-||P^Tx_i-P^Tx_k||^2)} \\] 记\\(g_{ij}=exp(-||P^T x_i-P^Tx_j||^2)\\)，则有 \\[p_{ij}=\\frac{g_{ij}}{\\sum\\limits_{k\\neq i}g_{ik}}\\] 由\\(\\frac{\\partial x^TD^TDx}{\\partial D}=2Dxx^T\\)得到 \\[ \\frac{\\partial g_{ij}}{\\partial A} = -2g_{ij}A(x_i-x_j)(x_i-x_j)^T \\] \\[\\begin{aligned} \\frac{\\partial p_{ij}}{\\partial A} \u0026amp;= \\frac{1}{(\\sum\\limits_{k\\neq i}g_{ik})^2}(\\frac{\\partial g_{ij}}{\\partial A}\\sum\\limits_{k \\neq i} g_{ik} -g_{ij}\\sum\\limits_{k \\neq i} \\frac{\\partial g_{ik}}{\\partial A}) \\\\ \u0026amp;= -2p_{ij}A \\Bigl( (x_i-x_j)(x_i-x_j)^Tp_{ij}A \\Bigl( (x_i-x_j)(x_i-x_j)^T- \\sum\\limits_{k \\neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T \\Bigr) \\end{aligned}\\] 目标梯度为 \\[\\begin{aligned} \\frac{\\partial f}{\\partial A} \u0026amp;= \\sum\\limits_{i=1}^n \\sum\\limits_{j \\in \\Omega_i} \\frac{\\partial p_{ij}}{\\partial A}\\\\ \u0026amp;= -2A\\sum\\limits_{i=1}^n \\sum\\limits_{j \\in \\Omega_i} p_{ij} \\Bigl( (x_i-x_j)(x_i-x_j)^T- \\sum\\limits_{k \\neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T \\Bigr) \\\\ \u0026amp;= -2A\\sum\\limits_{i=1}^n \\Bigl(p_i \\sum\\limits_{k \\neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T- \\sum\\limits_{j \\in \\Omega_i} p_{ij}(x_i-x_j)(x_i-x_j)^T \\Bigr) \\\\ \u0026amp;= 2\\sum\\limits_{i=1}^n \\sum\\limits_{j \\in \\Omega_i}(p_ip_{ij}-pmask_{ij})(Ax_i-Ax_j)(x_i-x_j)^T \\end{aligned}\\] 其中，\\(pmask_{ij}=p_{ij},if\\ j\\in \\Omega_i,else\\ 0\\). 记\\(W_{ij}=p_ip_{ij}-pmask_{ij}\\)，则有（简略推导） \\[ \\frac{\\partial f}{\\partial A} = 2(XA^T)^T(diag(sum(W,axis=0))-W-W^T)X \\]\n\n若已知某些样本相似和不相似，可定义“必连”约束集合 \\(\\mathcal M\\) 和 “勿连”约束集合 \\(\\mathcal C\\). \\[ \\mathop{min}\\limits_{M}\\quad \\sum\\limits_{(x_i,x_j) \\in \\mathcal M} ||x_i - x_j||^2_M \\\\ s.t.\\ \\sum\\limits_{(x_i,x_k) \\in \\mathcal C} ||x_i-x_k|| \\geqslant 1\\\\ M \\succeq 0 \\]\n其中，\\(M \\succeq 0\\) 表明 \\(M\\) 必须是半正定的，在不相似样本间距大于等于\\(1\\)的前提下使得相似样本的间距尽可能小。 \n （二）.文献分享  \n Schema: metric learning enables interpretable synthesis of heterogeneous single-cell modalities  \nThe author uses a principled metric learning strategy that identifies informative features in a modality to synthesize disparate modalities into a single coherent interpretation.\nSchema is designed for assays where multiple modalities are simultaneously measured for each cell. The researcher designates one highconfidence modality as the primary (i.e., reference) and one or more of the remaining modalities as secondary. Schema joints multimodality’s shared cells into single points in a space with associated distance metric that encapsulates modality-specific similarity between observations. Then it transforms the primary modality by scaling the weighted dimensions in order to have a desired correlation with remaining modalities. The new point locations represent information synthesized from multiple modalities.  Fig.1 Integration of simultaneously assayed modalities using Schema. \n Methods Correlation-based alignment and quadratic programming optimization \nIntuitive notion: pairwise distances in the transformed space can be highly correlated (Pearson correlation coefficient) with pairwise distances under each metric. \nMathematical formulation\n\\(D_j = \\{X_i^j：i=1,2, \\ldots,N;\\ j=1,2,\\ldots,r \\}\\).\n\\(N\\) deotes observations across \\(r\\) datasets \\(Dj\\).\nLet \\(D_1\\) be the primary dataset with \\(k\\) dimension.\n\\(\\rho_j(x_n^j,x_m^j)\\) is the squared distance between observations \\(n\\) and \\(m\\) in \\(D_j\\).\n\\(\\Omega\\) is limited to scaling transform. \\(\\Omega(D) = \\{diag(u)·x,x\\in D \\}\\), for \\(u \\in R^k\\) and \\(diag(u)\\) is a \\(k\\times k\\) diagonal matrix.\nThe squared distance between points under the transformation is given by \\[ \\rho^*(x_n,x_m) = ||diag(u)x_n- diag(u)x_m||^2 = diag(w)||x_n-x_m||^2 \\] where \\(w_i=u_i^2\\). And \\(u\\) acts as a feature-weighting mechanism: i.e., \\(u_i\\) being large means that the \\(i_th\\) coordinate of \\(D_1\\) is important.\nThe alignment between \\(\\rho^*\\) and \\(\\rho_j\\) is given by Pearson correlation. And the goal is to maximize the Corr \\[ Corr(\\rho^*,\\rho_j) = \\frac{Cov(\\rho^*,\\rho_j)}{(Var(\\rho^*)Var(\\rho_j))^{\\frac{1}{2}}} \\] Each modality could be weighted by user. Besides, the correlation allows a hard constraint. And the goal is \\[ \\Bigl\\{ \\sum\\limits_{j=2}^r Corr \\Bigl(\\rho^*(w),\\rho_j \\Bigr) \\Bigr\\} \\\\ Corr \\Bigl(\\rho^*(w),\\rho_1 \\Bigr) \\geqslant s \\] \\(\\rho^*\\) is the function of \\(w\\)\n\nSetting up the quadratic program\nGerenal quadratic programming (QP) is the solution of constrained least squares problem, the gernal form is \\[ \\mathop{min}\\ v^T Qv + q^T v \\\\ Gv \\leq h \\\\ Av=b \\] where \\(Q\\) is a positive semidefinite (psd) matrix.\nLet \\(w_i=u_i^2,w_i \\in R^k, \\delta_{ij}=(x_i-x_j)^2,\\delta_{ij} \\in R^k\\) and \\(P\\) be the set of pairs observations \\(P=\\{i,j\\},1 \\leqslant i \\leqslant j \\leqslant N\\).\nAs \\(Cov(X,Y) = E[X,Y]-E[X]E[Y], Var(X)=E[X^2]-W[X]^2\\) \\[ \\begin{aligned} Cov\\left(w,\\rho_l\\right) \u0026amp;=\\frac{1}{\\left|P\\right|}\\sum_{\\left\\{i,j\\right\\}\\in P}\\rho_l\\left(x_i^l, x_j^l\\right)\\delta_{ij}^Tw-\\frac{1}{\\left|P\\right|^2}\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^Tw \\sum_{\\left\\{i,j\\right\\}\\in P\\ }\\rho_l\\left(x_i^l, x_j^l\\right) \\\\ \u0026amp;= \\left(\\frac{1}{\\left|P\\right|}a_l-\\frac{1}{\\left|P\\right|^2}b_l\\right)^Tw\\ \\end{aligned} \\]\n\\[ \\begin{aligned} Var\\left(w\\right)\u0026amp;=\\frac{1}{\\left|P\\right|}\\sum_{\\left\\{i,j\\right\\}\\in P}w^T\\delta_{ij}\\delta_{ij}^Tw-\\frac{1}{\\left|P\\right|^2}\\left(\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^Tw\\right)^2\\ \\\\ \u0026amp;=w^T\\left(\\frac{1}{\\left|P\\right|}S-\\frac{1}{\\left|P\\right|^2}T\\right)w \\end{aligned} \\] where \\(a_l\\) and \\(b_l\\) are \\(k\\)-dimensional vectors depend only on \\(D_l\\),\\(T\\) and \\(T\\) are \\(N \\times k\\) depent on \\(D_1\\), \\[ a_l =\\sum_{\\left\\{i,j\\right\\}\\in P}\\rho_l\\left(x_i^{\\left(l\\right)},x_j^{\\left(l\\right)}\\right)\\delta_{ij} \\] \\[ b_l = (\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^Tw)(\\sum_{\\left\\{i,j\\right\\}\\in P\\ }\\rho_l\\left(x_i^l, x_j^l\\right)) \\] \\[ S= \\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}\\delta_{ij}^T \\] \\[ T=(\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij})(\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^T) \\]\nAs QP cann’t handle quotients or square roots. Maximizing a quotient can be reframed as maximizing the numerator (the covariance), minimizing the denominator (the variance), or both. So the optimization problem is \\[ \\sum_{j=1}^r\\ r_j Cov(w,\\rho_j)-\\alpha Var(w) -\\lambda||w-1||^2 \\\\ subject\\ to:\\ Cov(w,\\rho_1) \\geqslant \\beta \\\\ w \\succcurlyeq 0 \\] The mapping from the general optimization problem to the QP framework is as follows \\[ v =w \\\\ Q = \\frac{1}{|P|}S-\\frac{1}{|P|^2}T+ \\lambda I_k \\\\ q = -2\\lambda0-\\sum_{j=1}^r r_l (\\frac{1}{|P|}a_l- \\frac{1}{|P|^2}b_l) \\]\nUser can define the hyperparameters \\(s\\) and \\(t\\) \\[ Corr(\\rho^*,\\rho_1) \\geqslant s \\\\ \\frac{max\\{w\\}}{\\sum |w_i|} \\leqslant t \\] The choice of \\(\\lambda\\) controls whether \\(w\\) satisfies \\(t\\), and \\(\\alpha ,\\beta\\) controls whether \\(Corr\\) satisfies \\(s\\).\n\n\n参考链接：\n度量学习/对比学习入门: 论文阅读笔记-Deep Metric Learning Neighbourhood Component Analysis Schema: metric learning enables interpretable synthesis of heterogeneous single-cell modalities \n","permalink":"https://yupines.github.io/post/metric-learning/","summary":"在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。\n\n （一）.背景知识  \n 1.马氏距离（Mahalanobis Distance）  \n度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个\\(d\\)维样本\\(x_i,x_j\\)之间的马氏距离为\n\\[ dist_{mah}(x_i,x_j) = \\sqrt{(x_i - x_j)^T\\Sigma^{-1}(x_i - x_j)} \\] \\(\\Sigma\\) 为协方差矩阵.\n从欧式距离出发，\\(d\\)维样本\\(x_i,x_j\\)之间的欧式距离为 \\[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 = dist_{i,j,1}^2 + dist_{i,j,2}^2 + \\ldots + dist_{i,j,d}^2 \\] 对每一维\\(d\\)引入权重\\(w\\)得到 \\[\\begin{aligned} dist_{wed}^2(x_i,y_i) \u0026amp;= w_1·dist_{i,j,1}^2 + w_2·dist_{i,j,2}^2 + \\ldots + w_d·dist_{i,j,d}^2 \\\\ \u0026amp;= (x_i-x_j)^TM(x_i-x_j) \\end{aligned}\\] 其中\\(w \\geq 0,W=diag(w)，W\\)是一个对角矩阵.\n将\\(W\\)换成半正定对称矩阵\\(M\\)，即得到马氏距离. \\[ dist_{Mah} = ||x_i-x_j||^2_M \\] \\(M\\)即是度量矩阵，为使度量矩阵非负，\\(M\\)必须是（半）正定对称矩阵，满足\\(M=PP^T\\).\n马氏距离的几何意义:\n将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. \n 2.近邻分类器（neighbourhood Component Analysis, NCA）","title":"Metric Learning"},{"content":"PCA用于许多领域的数据分析中，考虑到各变量间存在一定相关性，将关系紧密的变量变成较少的综合指标来进行分析。即在原数据n维特征的基础上重构出全新的k维正交特征。第一个新坐标轴是原始数据中方差最大的方向，第二个新坐标轴是与第一个坐标轴正交的平面中使得方差最大的方向，以此类推，保留前K个特征维度。\n\n 1.协方差矩阵  \n样本均值：\\(\\bar{x} = \\frac{1}{n}\\sum\\limits_{i=1}^n\\ x_i\\)\n样本方差：\\(S^2 = \\frac{1}{n-1}\\sum\\limits_{i=1}^n\\ (x_i - \\bar{x})^2\\)\n样本\\(X\\)和\\(Y\\)的协方差：\\(\\begin{aligned} Cov(x,y) \u0026amp;= \\mathbb{E}[(X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y))] \\\\ \u0026amp;= \\frac{1}{n-1}\\sum\\limits_{i=1}^n\\ (x_i - \\bar{x})(y_i - \\bar{y}) \\end{aligned}\\) 协方差为0时，样本\\(X\\)和\\(Y\\)不具有线性相关性。\nn维\\(X=X_{1,3…n}\\) 的协方差矩阵为：\\[C = \\mathbb{E}[(X-\\bar{X})(X-\\bar{X})^T]]\\] 性质：协方差矩阵是对称矩阵，且半正定矩阵，主对角线是各个维度的方差。\n类间散度矩阵（scatter matrix）定义为： \\[S = \\sum\\limits_{k=1}^n\\ (X_k - \\bar{X_k})(X_k - \\bar{X_k})^T \\] 散度矩阵即是协方差矩阵 \\(C*(n-1)\\) \n 2.矩阵特征值和特征向量  \n 特征值与特征向量：\n\\(A\\)是\\(n\\)阶矩阵，若\\(Av=\\lambda v\\)，则\\(v\\)是矩阵\\(A\\)的特征向量，\\(\\lambda\\)是特征向量，系数行列式\\(|\\lambda E-A|\\)称为A的特征多项式，\\(E\\)为单位矩阵。\n\\[\\begin{aligned} \u0026amp; Av=\\lambda v\\quad\\Rightarrow\\quad (\\lambda E -A)v=0 \\quad\\Rightarrow\\quad \\\\ \u0026amp; |\\lambda E -A|= \\begin{vmatrix} \\lambda-a_{11} \u0026amp; -a_{12} \u0026amp; \\ldots \u0026amp; -a_{1n} \\\\ -a_{11} \u0026amp; \\lambda-a_{22} \u0026amp; \\ldots \u0026amp; -a_{2n} \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ -a_{m1} \u0026amp; -a_{m2} \u0026amp; \\ldots \u0026amp; \\lambda-a_{mn} \\end{vmatrix} =0 \\end{aligned}\\] 计算\\(|\\lambda E -A|=0\\)的行列式。 特征值分解： \\[A=Q\\Sigma Q^{-1} \\] 其中，Q是矩阵A的特征向量组成的矩阵，描述方向变换，\\(\\Sigma\\)为对角阵，对角线上的元素是特征值，由大到小排列，描述伸缩变换。\n\n 3.SVD分解  \nSVD适用于任意矩阵分解： \\[A=U\\Sigma V^T\\] 其中，A是\\(m*n\\)的矩阵，U则是\\(m*m\\)的方阵，称为左奇异向量，\\(V^T\\)是\\(n*n\\)的方阵，称为右奇异向量，\\(\\Sigma\\)上的值从大到小排序。\nSVD分解矩阵A的步骤：\n\\(AA^T\\)的特征向量构成构成\\(U\\)；\n\\(A^TA\\)的特征向量构成构成\\(V\\)；\n\\(A^TA\\)或者\\(AA^T\\)的特征值求平方根构成\\(\\Sigma\\).\n\n 4.PCA原理  \n构建超平面对所有样本进行恰当的表达，需满足：\na.最近重构性：样本点到超平面的距离足够近；\nb.最大可分性：样本点到超平面的投影足够小.\n在样本进行中心化后，假设投影变换后的新坐标为 \\(\\{w_1,w_2,w_3,\\ldots,w_d \\}\\)，\\(w_i\\) 是标准的正交基向量。若降维到 \\(d\u0026#39;\u0026lt; d\\)，则样本点在新空间内的投影为\\(z_i = (w_{i1},w_{i2},w_{i3},\\ldots,w_{id\u0026#39;})\\)，对于维度\\(j\\)有\\(x_i\\)的投影点的坐标为\\(z_{ij}=w_j^Tx_i\\)，基于\\(z_i\\)来重构\\(x\\)，则\\(\\hat{x}=\\sum\\limits_{j=1}^{d\u0026#39;}z_{ij}\\ w_j\\).  来自小曹老师的注解 基于最近重构性理解：\n若达到最大重构性，需满足重构点和样本点的距离和最小，\n已知\\(W^TW=I,z_i=W^Tx_i\\)，则 \\[\\begin{aligned} \\sum\\limits_{i=1}^m \\left\\Vert \\sum\\limits_{j=1}^{d\u0026#39;}z_{ij}w_j-x_i \\right\\Vert_2^2 \u0026amp;= \\sum\\limits_{i=1}^m \\left\\Vert Wz_i-x_i \\right\\Vert_2^2 \\\\ \u0026amp;= \\sum\\limits_{i=1}^m (Wz_i-x_i)^T(Wz_i-x_i) \\\\ \u0026amp;= \\sum\\limits_{i=1}^m (z_i^TW^TWz_i-z_i^TW^Tx_i-x_i^TWz_i+x^T_ix_i) \\\\ \u0026amp;= \\sum\\limits_{i=1}^m (z_i^TW^TWz_i-2z_i^TW^Tx_i+x^T_ix_i)\\\\ \u0026amp;= \\sum\\limits_{i=1}^m z_i^Tz_i -2\\sum\\limits_{i=1}^m z_i^TW^Tx_i+const \\\\ \u0026amp;= -\\sum\\limits_{i=1}^m z_i^Tz_i +const \\\\ \u0026amp;= -\\sum\\limits_{i=1}^m tr(z_iz_i^T) +const \\\\ \u0026amp;= -tr(\\sum\\limits_{i=1}^m z_iz_i^T) +const \\\\ \u0026amp;= -tr(\\sum\\limits_{i=1}^m W^Tx_ix_i^TW) +const \\\\ \u0026amp; \\propto tr(W^T(\\sum\\limits_{i=1}^m x_ix_i^T)W) \\end{aligned}\\] \\(\\sum\\limits_{i=1}^m x_ix_i^T\\)为协方差矩阵（严格为\\(\\frac{1}{m-1}\\)，可近似\\(\\frac{1}{m}\\)），因此PCA的优化目标为： \\[\\begin{aligned} \u0026amp; \\mathop{min}\\limits_{w}\\ -tr(W^TXX^TW) \\\\ \u0026amp; s.t.\\ W^TW=I \\end{aligned}\\]\n背景知识：拉格朗日乘子 设 \\(f:R^{n\\times m} \\to R,\\ \\Theta:R^{n\\times m} \\to R^{p\\times q}\\)，对于条件极值问题 \\(max\\ f(X) \\quad s.t\\ \\Theta(X) = 0\\) 或者 \\(min\\ f(X) \\quad s.t\\ \\Theta(X) = 0\\)，其\\(Lagrange\\)函数为： \\[L(X,Z) = f(X)+tr(Z^T\\Theta)\\] \\(Z\\)为大小与\\(\\Theta(X)\\)完全相同的矩阵，且每个元素都是\\(Lagrange\\)乘子.\n在\\(X\\in R^{d\\times m},W\\in R^{d\\times d\u0026#39;}, I\\in R^{d\u0026#39;\\times d\u0026#39;}\\)条件下，PCA优化目标的\\(Lagrange\\)函数为： \\[\\begin{aligned} L(W,\\Theta) \u0026amp;= -tr(W^TXX^TW)+\\langle \\Theta,W^TW-I \\rangle \\\\ \u0026amp;= -tr(W^TXX^TW)+tr(\\Theta(W^TW-I)) \\end{aligned}\\] 其中，\\(\\Theta \\in R^{d\u0026#39;\\times d\u0026#39;}\\)为\\(Lagrange\\)乘子矩阵，维度恒等于约束条件的维度，\\(\\langle \\Theta,W^TW-I \\rangle = tr(\\Theta(W^TW-I))\\)为矩阵的内积。若仅考虑\\(w^T_iw_i=1\\ (i=1,2,\\ldots,d\u0026#39;)\\)，则\\(\\Theta\\)为对角矩阵，令新的\\(Lagrange\\)乘子为\\(\\Lambda = diag(\\lambda_1,\\lambda_2,\\ldots,\\lambda_{d\u0026#39;}) \\in R^{d\u0026#39;\\times d\u0026#39;}\\)，则新的\\(Lagrange\\)函数为： \\[L(W,\\Lambda) = -tr(W^TXX^TW)+tr(\\Lambda^T(W^TW-I)) \\] 由微分公式\\(\\frac{\\partial}{\\partial X}tr(X^TBX)=BX+B^TX,\\ \\frac{\\partial}{\\partial X}tr(BX^TX)=XB^T+XB\\)， 对\\(W\\)求导可得到： \\[\\begin{aligned} \\frac{\\partial L(W,\\Lambda)}{\\partial,W} \u0026amp;= -\\frac{\\partial}{\\partial W} tr(W^TXX^TW) + \\frac{\\partial}{\\partial W} tr(\\Lambda^T(W^TW-I)) \\\\ \u0026amp;= -2XX^TW + W\\Lambda +W\\Lambda^T \\\\ \u0026amp;= -2XX^TW + 2W\\Lambda \\end{aligned}\\] 令\\(\\frac{\\partial L(W,\\Lambda)}{\\partial,W} =0\\) 可得到： \\[XX^TW=W\\Lambda\\] 将\\(W\\)和\\(\\Lambda\\)展开可得： \\[XX^Tw_i=\\lambda_i w_i,\\ i = 1,2,\\ldots,d\u0026#39; \\] 此式为矩阵特征值和特征向量的定义式，\\(\\lambda_i,w_i\\)分别为\\(XX^T\\)的特征值和特征向量，且\\(XX^T\\)为实对称矩阵，则\\(w\\)满足\\(w_i^Tw_i=1,w_i^Tw_i=0\\ (i\\ne j)\\)，将\\(XX^Tw_i=\\lambda_i w_i\\)代入目标函数可得到： \\[\\begin{aligned} \\mathop{min}\\limits_{w}\\ -tr(W^TXX^TW) \u0026amp;= \\mathop{max}\\limits_{w}\\ tr(W^TXX^TW) \\\\ \u0026amp;= \\mathop{max}\\limits_{w}\\ \\sum\\limits_{i=1}^{d\u0026#39;}w_i^TXX^Tw_i \\\\ \u0026amp;= \\mathop{max}\\limits_{w}\\ \\sum\\limits_{i=1}^{d\u0026#39;}w_i^T\\lambda_iw_i \\\\ \u0026amp;= \\mathop{max}\\limits_{w}\\ \\sum\\limits_{i=1}^{d\u0026#39;} \\lambda_i \\\\ \\end{aligned}\\] 基于最大可分性理解:\n投影后的样本点满足方差最大化，其协方差矩阵为\\(\\sum\\limits_{i}W^Tx_ix_i^TW\\)，于是优化目标可以写成 \\[\\begin{aligned} \u0026amp; \\mathop{max}\\limits_{w}\\ tr(W^TXX^TW) \\\\ \u0026amp; s.t.\\ W^TW=I \\end{aligned}\\] STEPS:\n中心化：每一特征减去各自的平均值\n 计算协方差矩阵：\\(XX^T\\)\n 求解\\(XX^T\\)的特征值和特征向量（待补充）\nd.将特征值从大到小排列，选取前\\(d\u0026#39;\\)个，将其对应的特征向量作为行向量组成矩阵\\(W^T\\)。\ne.数据变换\\(Z=W^TX\\) \n   5.Kernelized PCA  \n非线性降维的一种常用方法是，基于核技巧对线性降维方法进行“核化”，以核主成分分析（Kernelized PCA，KPCA）为例，其主要思想是将原始数据投影到更高维度，把原始数据向不同方向投影，便于找到合适的高维线性分类平面。\n将数据投影到\\(W\\)确定的平面： \\[ZZ^Tw_j=\\lambda_jw_j\\] \\(z_i\\)是样本在高维空间中的像点: \\[w_j=\\frac{1}{\\lambda_j}(\\sum\\limits_{i}^m z_iz_i^T)w_j = \\sum\\limits_{i}^m z_i \\frac{z_i^T w_j}{\\lambda_j} = \\sum\\limits_{i}^m z_i\\alpha_i^j = Z\\alpha^j \\] 其中 \\(\\alpha_i^j=\\frac{1}{\\lambda_j} z_i^Tw_j\\) 为\\(\\alpha_i\\)在\\(j\\)方向上的分量。将\\(w_j=Z\\alpha^j\\)代入上述公式可得： \\[\\begin{aligned} ZZ^TZ\\alpha^j \u0026amp;=\\lambda_j Z\\alpha^j \\\\ ZZ^TZ\\alpha^j \u0026amp;=Z \\lambda_j \\alpha^j \\end{aligned}\\] 目标是求出\\(w_j\\)，即满足条件的\\(\\alpha_j\\)，满足上式的\\(\\alpha_j\\)亦满足： \\[Z^TZ\\alpha^j = \\lambda_j \\alpha^j \\] 令\\(Z^TZ=K\\)，则 \\[K \\alpha^j = \\lambda_j \\alpha^j\\] 转变为求解\\(K\\)前\\(d\u0026#39;\\)个特征值和特征向量的问题。\n假设\\(z_i\\)由原始样本点\\(x_i\\)通过映射\\(\\phi\\)得到，则有\\(z_i=\\phi(x_i)\\)。一般情况下，我们不清楚\\(\\phi\\)的分布，故引入核函数： \\[ k(i,j) = \\langle \\phi(i),\\phi(j) \\rangle = \\phi(i)^T\\phi(j) \\] 矩阵\\(K\\)的第\\(i\\)行\\(j\\)元素即为 \\(K_{ij}=z_i^Tz_j=k(i,j)\\).\n对于新的样本\\(x\\)，其投影后的第\\(j\\)维坐标为 \\[ z_j = w^T_j \\phi(x) = \\sum\\limits_{i=1}^m \\alpha_i^j \\phi(x_i)^T \\phi(x) = \\sum\\limits_{i=1}^m \\alpha_i^j k(i,j) \\]\n\n参考链接：\nhttps://zhuanlan.zhihu.com/p/37777074 https://datawhalechina.github.io/pumpkin-book/#/chapter10/chapter10?id=_1014\nhttps://blog.csdn.net/Junerror/article/details/80222540\nhttps://zhuanlan.zhihu.com/p/59775730\n\n","permalink":"https://yupines.github.io/post/pca-svd/","summary":"PCA用于许多领域的数据分析中，考虑到各变量间存在一定相关性，将关系紧密的变量变成较少的综合指标来进行分析。即在原数据n维特征的基础上重构出全新的k维正交特征。第一个新坐标轴是原始数据中方差最大的方向，第二个新坐标轴是与第一个坐标轴正交的平面中使得方差最大的方向，以此类推，保留前K个特征维度。\n\n 1.协方差矩阵  \n样本均值：\\(\\bar{x} = \\frac{1}{n}\\sum\\limits_{i=1}^n\\ x_i\\)\n样本方差：\\(S^2 = \\frac{1}{n-1}\\sum\\limits_{i=1}^n\\ (x_i - \\bar{x})^2\\)\n样本\\(X\\)和\\(Y\\)的协方差：\\(\\begin{aligned} Cov(x,y) \u0026amp;= \\mathbb{E}[(X-\\mathbb{E}(X))(Y-\\mathbb{E}(Y))] \\\\ \u0026amp;= \\frac{1}{n-1}\\sum\\limits_{i=1}^n\\ (x_i - \\bar{x})(y_i - \\bar{y}) \\end{aligned}\\) 协方差为0时，样本\\(X\\)和\\(Y\\)不具有线性相关性。\nn维\\(X=X_{1,3…n}\\) 的协方差矩阵为：\\[C = \\mathbb{E}[(X-\\bar{X})(X-\\bar{X})^T]]\\] 性质：协方差矩阵是对称矩阵，且半正定矩阵，主对角线是各个维度的方差。\n类间散度矩阵（scatter matrix）定义为： \\[S = \\sum\\limits_{k=1}^n\\ (X_k - \\bar{X_k})(X_k - \\bar{X_k})^T \\] 散度矩阵即是协方差矩阵 \\(C*(n-1)\\) \n 2.矩阵特征值和特征向量  \n 特征值与特征向量：\n\\(A\\)是\\(n\\)阶矩阵，若\\(Av=\\lambda v\\)，则\\(v\\)是矩阵\\(A\\)的特征向量，\\(\\lambda\\)是特征向量，系数行列式\\(|\\lambda E-A|\\)称为A的特征多项式，\\(E\\)为单位矩阵。\n\\[\\begin{aligned} \u0026amp; Av=\\lambda v\\quad\\Rightarrow\\quad (\\lambda E -A)v=0 \\quad\\Rightarrow\\quad \\\\ \u0026amp; |\\lambda E -A|= \\begin{vmatrix} \\lambda-a_{11} \u0026amp; -a_{12} \u0026amp; \\ldots \u0026amp; -a_{1n} \\\\ -a_{11} \u0026amp; \\lambda-a_{22} \u0026amp; \\ldots \u0026amp; -a_{2n} \\\\ \\ldots \u0026amp; \\ldots \u0026amp; \\ldots \\\\ -a_{m1} \u0026amp; -a_{m2} \u0026amp; \\ldots \u0026amp; \\lambda-a_{mn} \\end{vmatrix} =0 \\end{aligned}\\] 计算\\(|\\lambda E -A|=0\\)的行列式。 特征值分解： \\[A=Q\\Sigma Q^{-1} \\] 其中，Q是矩阵A的特征向量组成的矩阵，描述方向变换，\\(\\Sigma\\)为对角阵，对角线上的元素是特征值，由大到小排列，描述伸缩变换。","title":"Principle Component Analysis"},{"content":"经过一段时间东拼西凑的学习，小白终于能理解变分自编码器了哈哈哈，感谢各路大神的教程！在这里整理一些简短的笔记。\n一、VAE的初步理解  1. 给定真实样本\\(X_k\\),假设存在专属于\\(X_k\\)的后验分布\\(p(Z|X_k)\\),并假定这个分布是正态多元的，专属的目的是训练生成器\\(X = g(Z)\\),从分布\\(p(Z|X_k)\\)采样出一个\\(Z_k\\)并还原为\\(\\hat{X_k}\\)。\n2. 使用神经网络拟合正态分布的参数：均值\\(\\mu\\)和方差\\({\\sigma}^2\\)。于是构建两个神经网络：\\({\\mu}k = {f}_1(X_k)\\)和\\({\\log}{\\sigma}_k^2 = {f}_2(X_k)\\) （不需要激活函数的处理）。\n3. 重构X的过程需最小化\\((\\hat{X_k},X_k)\\)的距离，重构噪声（即方差）既要尽可能地小（减少重构误差），同时又要防止退化为0（保证模型具有生成能力），那么可以让所有样本的\\(p(Z|X_k)\\)向标准正态分布看齐。若接近正态分布\\(N(0,I)\\)，则： \\[p(Z) = \\sum\\limits_{X}p(Z|X)p(X) = \\sum N(0,I)p(X) = N(0,I)\\] \\(p(Z)\\)满足正态分布。接下来利用KL散度来计算loss，即\\(KL(N(\\mu,{\\sigma}^2)||N(0,I))\\)，计算结果为：\n\\[\\mathcal{L}_{\\mu,{\\sigma}^2} = \\frac{1}{2}\\sum\\limits_{i=1}^d({\\mu}_{(i)}^2 + {\\sigma}^2_{(i)}-{\\log}{\\sigma}^2_{(i)}-1) \\] 其中\\(d\\)是因变量\\(Z\\)的维度。\n \n4. 重参数技巧：从正态分布中采样 \\(\\varepsilon\\)，然后\\(Z = \\mu + \\varepsilon * \\sigma\\)  二、VAE的目标函数  （一）背景知识  1.边缘分布  假设\\(p(x,y)\\),其中一个特定变量的边缘分布为： \\[p(x) = \\sum\\limits_{y}p(x,y) = \\sum\\limits_{y}p(x|y)p(y)\\]  2.交叉熵和KL散度  交叉熵和KL关系密切，可以用来衡量两个分布的差异，以离散变量\\(x\\)为例： \\[H(P,Q)=-\\mathbb{E}_{x\\sim P}\\ {\\log}Q(x) = -\\sum\\limits_{i=1}^N P(x_i){\\log}Q(x_i)\\] \\(KL\\)散度定义，对\\(P,Q\\)分布有： \\[D_{KL}(P||Q) = \\mathbb{E}_{x\\sim P}[{\\log}\\frac{P(x)}{Q(x)}] = \\mathbb{E}_{x\\sim P}[{\\log}P(x) - {\\log}Q(x)] \\] 对于离散型变量有： \\[D_{KL}(P||Q) = \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}(\\frac{P(x_i)}{Q(x_i)}) = \\sum\\limits_{i=1}^N P(x_i)\\ [{\\log}P(x_i)-{\\log}Q(x_i)] \\] 对于连续型变量，其概率密度函数（probability density function，PDF）用小写字母\\(p(x)\\)和\\(q(x)\\)来表示，离散型随机变量的概率质量函数（probability mass function，PMF）用大写字母表示。\n以离散变量为例，对于\\(KL\\)散度公式展开可得到： \\[\\begin{aligned} D_{KL}(P||Q) \u0026amp;= \\sum\\limits_{i=1}^N P(x_i)\\ [{\\log}P(x_i)-{\\log}Q(x_i)] \\\\ \u0026amp;= \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}P(x_i) - \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}Q(x_i) \\\\ \u0026amp;= -[-\\sum\\limits_{i=1}^N P(x_i)\\ {\\log}P(x_i)]+[-\\sum\\limits_{i=1}^N P(x_i)\\ {\\log}Q(x_i)] \\\\ \u0026amp;=-H(P) + H(P,Q) \\end{aligned}\\] 即\\(H(P,Q) = H(P) + D_{KL}(P||Q)\\)  3.EM算法  Expectation Maximization Algorithm,一种迭代算法，用于含有隐变量的概率参数模型的最大似然估计或极大后验概率估计。分为Expection-Step 和 Maximization-Step。E-Step 主要通过观察数据和现有模型来估计参数，由此计算似然函数的期望值；M-Step 寻找似然函数最大时对应的参数。\nEM算法的一般思路： \\[\\begin{aligned} L(\\theta) \u0026amp;= \\mathbb{E}_{x\\sim data}[{\\log}\\ p_{\\theta}(x)] \\\\ \u0026amp;= \\mathbb{E}_{x\\sim data}[{\\log}\\ p_{\\theta}(z,x)-{\\log}\\ p_{\\theta}(z|x)]\\\\ \u0026amp;= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z,x)]-\\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z|x)] \\\\ \u0026amp;= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z,x)] + H(\\phi) + KL(\\phi||\\theta) \\end{aligned}\\] 引入任意分布\\(z\\)和参数\\(\\theta,\\forall \\phi\\) STEPS：初始化参数\\(\\theta\\)，重复计算E和M步骤\nE-Step：根据\\(p_{\\theta}(z)\\)和\\(p_{\\theta}(x|z)\\)求出reference，即\\(p_{\\theta}(x|z)\\)，令\\({\\phi}={\\theta}\\); M-Step：固定\\(\\phi\\)后，寻找新的参数\\(\\theta\\)，使得\\(\\mathbb{E}_{x\\sim data}[{\\log}_{\\theta}(x)]\\)增大（即‘辅助函数’）。 在迭代中，\\(\\mathbb{E}_{x\\sim data}[{\\log}\\ p_{\\theta}(x)]\\)不断增大，\\(H(\\phi)\\)与\\(\\theta\\)无关，\\(KL(\\phi||\\theta) = 0\\) 变成了 \\(KL(\\phi||\\theta)\u0026gt;0\\)，于是随着迭代进行，目标函数\\(L(\\theta)\\)不断增大。  （二）从概率图模型的角度理解   VAE的训练近似EM  假设一个概率图模型，可以生成隐变量\\(z\\)，然后生成观测变量\\(x\\)，参数为\\(\\theta\\)。得到分布：\\(p_{\\theta}(z)\\)和\\(p_{\\theta}(x|z)\\)。由此推测隐藏分布：\\(x\\)的边缘分布\\(p_{\\theta}(x)\\)和\\(z\\)的条件分布\\(p_{\\theta}(z|x)\\)。模型可以采用最大似然法训练，找到使得\\(L(\\theta) = \\mathbb{E}_{x\\sim data}{\\log}\\ p_{\\theta}(x)\\)最大的参数\\(\\theta\\)。\n对于VAE来说，\\(p_{\\theta}(z)\\) 固定为\\(N(0,I)\\)，而\\(p_{\\theta}(x|z)\\) 由decoder定义，于是EM算法中的目标函数变成： \\[\\begin{aligned} L(\\theta) \u0026amp;= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z,x)] + H(\\phi) + KL(\\phi||\\theta) \\\\ \u0026amp;= \\mathbb{E}_{\\phi}[{\\log}\\ p_{\\theta}(z)\\ +{\\log}\\ p_{\\theta}(x|z)] + H(\\phi) + KL(\\phi||\\theta) \\\\ \u0026amp;= \\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -[H(\\phi)+KL(\\phi||p_{\\theta}(z))]+ H(\\phi) + KL(\\phi||\\theta) \\\\ \u0026amp;= \\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -KL(\\phi||p_{\\theta}(z))+ KL(\\phi||\\theta) \\end{aligned}\\] 此时，理想的EM算法步骤为：\nE-Step：令\\(\\phi\\)为概率图模型中隐含的条件分布\\(p_{\\theta}(z|x)\\)；\nS-Step：寻找新的\\(\\theta\\)使得\\(L(\\theta)\\)继续增大。\n但对于VAE而言，\\(p_{\\theta}(z|x)\\)无法显示表达出来，于是VAE设计一个参数为\\(\\phi\\)的网络来近似\\(\\theta\\)。实际EM算法如下：\nE-Step：令\\(\\phi\\)逼近 \\(\\theta\\)；\nM-Step：寻找 \\(\\theta\\) 来增大 \\(\\mathbb{E}_{\\phi}{\\log}\\) 可以看出迭代的过程主要使 \\(\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -KL(\\phi||p_{\\theta}(z))\\) 增大，使 \\(KL(\\phi||\\theta)\\) 逼近零。\n于是定义：Evidence Lower BOund（ELBO） \\[ELBO(\\theta,\\phi) = \\mathbb{E}_{x\\sim data}[\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z)\\ -\\ KL({\\phi}||N(0,I))] \\] evidence指的是目标函数 \\({\\log}\\ p_{\\theta}(x)\\)，因为 \\(KL(\\phi||\\theta)\\) 非负，所以前两项为目标函数的下界。ELBO即为VAE的目标函数，由于E-M步增大的目标是相同的，可以合并，用梯度上升法来最大化ELBO，最大化目标函数的下界，以此间接最大化目标函数。 （三）从神经网络的角度理解  VAE是带有正则化的自编码器  普通自编码器由编码器 \\(z = g_{\\phi}(x)\\) 和解码器 \\(x = f_{\\theta}(z)\\) 构成。目标函数是最小化 \\(D(x,f_{\\theta}(g_{\\phi}(x)))\\)。若\\(x\\)是二值，则 \\(f_{\\theta}(z)\\) 代表伯努利分布均值；若\\(x\\)是任意实数，则 \\(f_{\\theta}(z)\\) 代表正态分布，重构误差可以写成 \\(-{\\log}\\ p_{\\theta}(x|z)\\)，最大似然训练等价于最小化重构误差: \\[L_{reconstruct}(\\theta,\\phi) = -\\ \\mathbb{E}_{x\\sim data}\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z) \\] VAE除了重构，还要考虑生成和插值的问题，需要让编码器的输出\\(z\\)分布服从\\(N(0,I)\\)，于是添加正则项： \\[L_{reg}(\\theta,\\phi) = KL(\\mathbb{E}_{x\\sim data}\\ p_{\\phi}(z|x)||N(0,I)) \\] 因有 \\(KL(\\mathbb{E}_p||q)\\leq \\mathbb{E}KL(p||q)\\)，则VAE的总损失函数为：\n\\[\\begin{aligned} L(\\theta,\\phi) \u0026amp;=L_{reconstruct}(\\theta,\\phi)+L_{reg}(\\theta,\\phi)\\\\ \u0026amp;= -\\ \\mathbb{E}_{x\\sim data}\\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z) + KL(\\mathbb{E}_{x\\sim data}\\ p_{\\phi}(z|x)||N(0,I)) \\\\ \u0026amp;\\leq \\mathbb{E}_{x\\sim data}[- \\mathbb{E}_{\\phi}{\\log}\\ p_{\\theta}(x|z) + KL(p_{\\phi}(z|x)||N(0,I))] \\end{aligned}\\] 即得到 ELBO！(o゜▽゜)o☆ BINGO!  参考链接：\nhttps://kexue.fm/archives/5253 https://zhuanlan.zhihu.com/p/366204765\nhttps://zhuanlan.zhihu.com/p/78311644\n\n","permalink":"https://yupines.github.io/post/vae/","summary":"经过一段时间东拼西凑的学习，小白终于能理解变分自编码器了哈哈哈，感谢各路大神的教程！在这里整理一些简短的笔记。\n一、VAE的初步理解  1. 给定真实样本\\(X_k\\),假设存在专属于\\(X_k\\)的后验分布\\(p(Z|X_k)\\),并假定这个分布是正态多元的，专属的目的是训练生成器\\(X = g(Z)\\),从分布\\(p(Z|X_k)\\)采样出一个\\(Z_k\\)并还原为\\(\\hat{X_k}\\)。\n2. 使用神经网络拟合正态分布的参数：均值\\(\\mu\\)和方差\\({\\sigma}^2\\)。于是构建两个神经网络：\\({\\mu}k = {f}_1(X_k)\\)和\\({\\log}{\\sigma}_k^2 = {f}_2(X_k)\\) （不需要激活函数的处理）。\n3. 重构X的过程需最小化\\((\\hat{X_k},X_k)\\)的距离，重构噪声（即方差）既要尽可能地小（减少重构误差），同时又要防止退化为0（保证模型具有生成能力），那么可以让所有样本的\\(p(Z|X_k)\\)向标准正态分布看齐。若接近正态分布\\(N(0,I)\\)，则： \\[p(Z) = \\sum\\limits_{X}p(Z|X)p(X) = \\sum N(0,I)p(X) = N(0,I)\\] \\(p(Z)\\)满足正态分布。接下来利用KL散度来计算loss，即\\(KL(N(\\mu,{\\sigma}^2)||N(0,I))\\)，计算结果为：\n\\[\\mathcal{L}_{\\mu,{\\sigma}^2} = \\frac{1}{2}\\sum\\limits_{i=1}^d({\\mu}_{(i)}^2 + {\\sigma}^2_{(i)}-{\\log}{\\sigma}^2_{(i)}-1) \\] 其中\\(d\\)是因变量\\(Z\\)的维度。\n \n4. 重参数技巧：从正态分布中采样 \\(\\varepsilon\\)，然后\\(Z = \\mu + \\varepsilon * \\sigma\\)  二、VAE的目标函数  （一）背景知识  1.边缘分布  假设\\(p(x,y)\\),其中一个特定变量的边缘分布为： \\[p(x) = \\sum\\limits_{y}p(x,y) = \\sum\\limits_{y}p(x|y)p(y)\\]  2.交叉熵和KL散度  交叉熵和KL关系密切，可以用来衡量两个分布的差异，以离散变量\\(x\\)为例： \\[H(P,Q)=-\\mathbb{E}_{x\\sim P}\\ {\\log}Q(x) = -\\sum\\limits_{i=1}^N P(x_i){\\log}Q(x_i)\\] \\(KL\\)散度定义，对\\(P,Q\\)分布有： \\[D_{KL}(P||Q) = \\mathbb{E}_{x\\sim P}[{\\log}\\frac{P(x)}{Q(x)}] = \\mathbb{E}_{x\\sim P}[{\\log}P(x) - {\\log}Q(x)] \\] 对于离散型变量有： \\[D_{KL}(P||Q) = \\sum\\limits_{i=1}^N P(x_i)\\ {\\log}(\\frac{P(x_i)}{Q(x_i)}) = \\sum\\limits_{i=1}^N P(x_i)\\ [{\\log}P(x_i)-{\\log}Q(x_i)] \\] 对于连续型变量，其概率密度函数（probability density function，PDF）用小写字母\\(p(x)\\)和\\(q(x)\\)来表示，离散型随机变量的概率质量函数（probability mass function，PMF）用大写字母表示。","title":"Variational Auto-Encoder"}]