<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Metric Learning | yupines</title>
<meta name="keywords" content="" />
<meta name="description" content="在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。

 （一）.背景知识  
 1.马氏距离（Mahalanobis Distance）  
度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个\(d\)维样本\(x_i,x_j\)之间的马氏距离为
\[ dist_{mah}(x_i,x_j) = \sqrt{(x_i - x_j)^T\Sigma^{-1}(x_i - x_j)} \] \(\Sigma\) 为协方差矩阵.
从欧式距离出发，\(d\)维样本\(x_i,x_j\)之间的欧式距离为 \[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 = dist_{i,j,1}^2 &#43; dist_{i,j,2}^2 &#43; \ldots &#43; dist_{i,j,d}^2 \] 对每一维\(d\)引入权重\(w\)得到 \[\begin{aligned} dist_{wed}^2(x_i,y_i) &amp;= w_1·dist_{i,j,1}^2 &#43; w_2·dist_{i,j,2}^2 &#43; \ldots &#43; w_d·dist_{i,j,d}^2 \\ &amp;= (x_i-x_j)^TM(x_i-x_j) \end{aligned}\] 其中\(w \geq 0,W=diag(w)，W\)是一个对角矩阵.
将\(W\)换成半正定对称矩阵\(M\)，即得到马氏距离. \[ dist_{Mah} = ||x_i-x_j||^2_M \] \(M\)即是度量矩阵，为使度量矩阵非负，\(M\)必须是（半）正定对称矩阵，满足\(M=PP^T\).
马氏距离的几何意义:
将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. 
 2.近邻分类器（neighbourhood Component Analysis, NCA）">
<meta name="author" content="">
<link rel="canonical" href="https://yupines.github.io/post/metric-learning/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css" integrity="sha256-yIlj/i15RiAA/Q&#43;xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://yupines.github.io/icon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yupines.github.io/icon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yupines.github.io/icon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yupines.github.io/icon/apple-touch-icon.png">
<link rel="mask-icon" href="https://yupines.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.92.1" />
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Metric Learning" />
<meta property="og:description" content="在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。

 （一）.背景知识  
 1.马氏距离（Mahalanobis Distance）  
度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个\(d\)维样本\(x_i,x_j\)之间的马氏距离为
\[ dist_{mah}(x_i,x_j) = \sqrt{(x_i - x_j)^T\Sigma^{-1}(x_i - x_j)} \] \(\Sigma\) 为协方差矩阵.
从欧式距离出发，\(d\)维样本\(x_i,x_j\)之间的欧式距离为 \[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 = dist_{i,j,1}^2 &#43; dist_{i,j,2}^2 &#43; \ldots &#43; dist_{i,j,d}^2 \] 对每一维\(d\)引入权重\(w\)得到 \[\begin{aligned} dist_{wed}^2(x_i,y_i) &amp;= w_1·dist_{i,j,1}^2 &#43; w_2·dist_{i,j,2}^2 &#43; \ldots &#43; w_d·dist_{i,j,d}^2 \\ &amp;= (x_i-x_j)^TM(x_i-x_j) \end{aligned}\] 其中\(w \geq 0,W=diag(w)，W\)是一个对角矩阵.
将\(W\)换成半正定对称矩阵\(M\)，即得到马氏距离. \[ dist_{Mah} = ||x_i-x_j||^2_M \] \(M\)即是度量矩阵，为使度量矩阵非负，\(M\)必须是（半）正定对称矩阵，满足\(M=PP^T\).
马氏距离的几何意义:
将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. 
 2.近邻分类器（neighbourhood Component Analysis, NCA）" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yupines.github.io/post/metric-learning/" /><meta property="og:image" content="https://yupines.github.io/papermod-cover.png"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-03-11T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-03-11T00:00:00&#43;00:00" /><meta property="og:site_name" content="yupines" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://yupines.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Metric Learning"/>
<meta name="twitter:description" content="在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。

 （一）.背景知识  
 1.马氏距离（Mahalanobis Distance）  
度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个\(d\)维样本\(x_i,x_j\)之间的马氏距离为
\[ dist_{mah}(x_i,x_j) = \sqrt{(x_i - x_j)^T\Sigma^{-1}(x_i - x_j)} \] \(\Sigma\) 为协方差矩阵.
从欧式距离出发，\(d\)维样本\(x_i,x_j\)之间的欧式距离为 \[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 = dist_{i,j,1}^2 &#43; dist_{i,j,2}^2 &#43; \ldots &#43; dist_{i,j,d}^2 \] 对每一维\(d\)引入权重\(w\)得到 \[\begin{aligned} dist_{wed}^2(x_i,y_i) &amp;= w_1·dist_{i,j,1}^2 &#43; w_2·dist_{i,j,2}^2 &#43; \ldots &#43; w_d·dist_{i,j,d}^2 \\ &amp;= (x_i-x_j)^TM(x_i-x_j) \end{aligned}\] 其中\(w \geq 0,W=diag(w)，W\)是一个对角矩阵.
将\(W\)换成半正定对称矩阵\(M\)，即得到马氏距离. \[ dist_{Mah} = ||x_i-x_j||^2_M \] \(M\)即是度量矩阵，为使度量矩阵非负，\(M\)必须是（半）正定对称矩阵，满足\(M=PP^T\).
马氏距离的几何意义:
将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. 
 2.近邻分类器（neighbourhood Component Analysis, NCA）"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yupines.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Metric Learning",
      "item": "https://yupines.github.io/post/metric-learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Metric Learning",
  "name": "Metric Learning",
  "description": "在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。\n\n （一）.背景知识  \n 1.马氏距离（Mahalanobis Distance）  \n度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个\\(d\\)维样本\\(x_i,x_j\\)之间的马氏距离为\n\\[ dist_{mah}(x_i,x_j) = \\sqrt{(x_i - x_j)^T\\Sigma^{-1}(x_i - x_j)} \\] \\(\\Sigma\\) 为协方差矩阵.\n从欧式距离出发，\\(d\\)维样本\\(x_i,x_j\\)之间的欧式距离为 \\[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 = dist_{i,j,1}^2 + dist_{i,j,2}^2 + \\ldots + dist_{i,j,d}^2 \\] 对每一维\\(d\\)引入权重\\(w\\)得到 \\[\\begin{aligned} dist_{wed}^2(x_i,y_i) \u0026amp;= w_1·dist_{i,j,1}^2 + w_2·dist_{i,j,2}^2 + \\ldots + w_d·dist_{i,j,d}^2 \\\\ \u0026amp;= (x_i-x_j)^TM(x_i-x_j) \\end{aligned}\\] 其中\\(w \\geq 0,W=diag(w)，W\\)是一个对角矩阵.\n将\\(W\\)换成半正定对称矩阵\\(M\\)，即得到马氏距离. \\[ dist_{Mah} = ||x_i-x_j||^2_M \\] \\(M\\)即是度量矩阵，为使度量矩阵非负，\\(M\\)必须是（半）正定对称矩阵，满足\\(M=PP^T\\).\n马氏距离的几何意义:\n将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. \n 2.近邻分类器（neighbourhood Component Analysis, NCA）",
  "keywords": [
    
  ],
  "articleBody": "在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。\n\n （一）.背景知识  \n 1.马氏距离（Mahalanobis Distance）  \n度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个\\(d\\)维样本\\(x_i,x_j\\)之间的马氏距离为\n\\[ dist_{mah}(x_i,x_j) = \\sqrt{(x_i - x_j)^T\\Sigma^{-1}(x_i - x_j)} \\] \\(\\Sigma\\) 为协方差矩阵.\n从欧式距离出发，\\(d\\)维样本\\(x_i,x_j\\)之间的欧式距离为 \\[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 = dist_{i,j,1}^2 + dist_{i,j,2}^2 + \\ldots + dist_{i,j,d}^2 \\] 对每一维\\(d\\)引入权重\\(w\\)得到 \\[\\begin{aligned} dist_{wed}^2(x_i,y_i) \u0026= w_1·dist_{i,j,1}^2 + w_2·dist_{i,j,2}^2 + \\ldots + w_d·dist_{i,j,d}^2 \\\\ \u0026= (x_i-x_j)^TM(x_i-x_j) \\end{aligned}\\] 其中\\(w \\geq 0,W=diag(w)，W\\)是一个对角矩阵.\n将\\(W\\)换成半正定对称矩阵\\(M\\)，即得到马氏距离. \\[ dist_{Mah} = ||x_i-x_j||^2_M \\] \\(M\\)即是度量矩阵，为使度量矩阵非负，\\(M\\)必须是（半）正定对称矩阵，满足\\(M=PP^T\\).\n马氏距离的几何意义:\n将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. \n 2.近邻分类器（neighbourhood Component Analysis, NCA）  \n对\\(M\\)的学习需要设定目标. 以近邻分类器为例，假设希望提高NCA的性能，则可将\\(M\\)嵌入到NCA的评价指标.\n计算样本\\(i\\)的近邻分布 \\[ p_{ij} = \\frac{exp(-||x_i-x_j||^2_M)}{\\sum\\limits_{k\\neq i}exp(-||x_i-x_k||^2_M)} \\] \\(x_j\\)对\\(x_i\\)的影响随着距离的增大而减少. 以留一法(LOO)正确率最大化为目标，计算\\(x_i\\)的留一法正确率 \\[ p_i = \\sum\\limits_{j \\in \\Omega_i} p_{ij} \\] \\(\\Omega_i\\)表示与\\(x_i\\)同类别的样本的下标集合，整个样本集的留一法正确率为 \\[\\sum\\limits_{i=1}^m p_i=\\sum\\limits_{i=1}^m \\sum\\limits_{j \\in \\Omega_i} p_{ij} \\] 将\\(p_{ij}\\)及\\(M=PP^T\\)代入上式可得NCA的优化目标为 \\[ \\mathop{min}\\limits_{P}\\quad 1- \\sum\\limits_{i=1}^m \\sum\\limits_{j \\in \\Omega_i} \\frac{exp(-||P^T x_i-P^Tx_j||^2)}{\\sum\\limits_{k\\neq i}exp(-||P^Tx_i-P^Tx_k||^2)} \\] 记\\(g_{ij}=exp(-||P^T x_i-P^Tx_j||^2)\\)，则有 \\[p_{ij}=\\frac{g_{ij}}{\\sum\\limits_{k\\neq i}g_{ik}}\\] 由\\(\\frac{\\partial x^TD^TDx}{\\partial D}=2Dxx^T\\)得到 \\[ \\frac{\\partial g_{ij}}{\\partial A} = -2g_{ij}A(x_i-x_j)(x_i-x_j)^T \\] \\[\\begin{aligned} \\frac{\\partial p_{ij}}{\\partial A} \u0026= \\frac{1}{(\\sum\\limits_{k\\neq i}g_{ik})^2}(\\frac{\\partial g_{ij}}{\\partial A}\\sum\\limits_{k \\neq i} g_{ik} -g_{ij}\\sum\\limits_{k \\neq i} \\frac{\\partial g_{ik}}{\\partial A}) \\\\ \u0026= -2p_{ij}A \\Bigl( (x_i-x_j)(x_i-x_j)^Tp_{ij}A \\Bigl( (x_i-x_j)(x_i-x_j)^T- \\sum\\limits_{k \\neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T \\Bigr) \\end{aligned}\\] 目标梯度为 \\[\\begin{aligned} \\frac{\\partial f}{\\partial A} \u0026= \\sum\\limits_{i=1}^n \\sum\\limits_{j \\in \\Omega_i} \\frac{\\partial p_{ij}}{\\partial A}\\\\ \u0026= -2A\\sum\\limits_{i=1}^n \\sum\\limits_{j \\in \\Omega_i} p_{ij} \\Bigl( (x_i-x_j)(x_i-x_j)^T- \\sum\\limits_{k \\neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T \\Bigr) \\\\ \u0026= -2A\\sum\\limits_{i=1}^n \\Bigl(p_i \\sum\\limits_{k \\neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T- \\sum\\limits_{j \\in \\Omega_i} p_{ij}(x_i-x_j)(x_i-x_j)^T \\Bigr) \\\\ \u0026= 2\\sum\\limits_{i=1}^n \\sum\\limits_{j \\in \\Omega_i}(p_ip_{ij}-pmask_{ij})(Ax_i-Ax_j)(x_i-x_j)^T \\end{aligned}\\] 其中，\\(pmask_{ij}=p_{ij},if\\ j\\in \\Omega_i,else\\ 0\\). 记\\(W_{ij}=p_ip_{ij}-pmask_{ij}\\)，则有（简略推导） \\[ \\frac{\\partial f}{\\partial A} = 2(XA^T)^T(diag(sum(W,axis=0))-W-W^T)X \\]\n\n若已知某些样本相似和不相似，可定义“必连”约束集合 \\(\\mathcal M\\) 和 “勿连”约束集合 \\(\\mathcal C\\). \\[ \\mathop{min}\\limits_{M}\\quad \\sum\\limits_{(x_i,x_j) \\in \\mathcal M} ||x_i - x_j||^2_M \\\\ s.t.\\ \\sum\\limits_{(x_i,x_k) \\in \\mathcal C} ||x_i-x_k|| \\geqslant 1\\\\ M \\succeq 0 \\]\n其中，\\(M \\succeq 0\\) 表明 \\(M\\) 必须是半正定的，在不相似样本间距大于等于\\(1\\)的前提下使得相似样本的间距尽可能小。 \n （二）.文献分享  \n Schema: metric learning enables interpretable synthesis of heterogeneous single-cell modalities  \nThe author uses a principled metric learning strategy that identifies informative features in a modality to synthesize disparate modalities into a single coherent interpretation.\nSchema is designed for assays where multiple modalities are simultaneously measured for each cell. The researcher designates one highconfidence modality as the primary (i.e., reference) and one or more of the remaining modalities as secondary. Schema joints multimodality’s shared cells into single points in a space with associated distance metric that encapsulates modality-specific similarity between observations. Then it transforms the primary modality by scaling the weighted dimensions in order to have a desired correlation with remaining modalities. The new point locations represent information synthesized from multiple modalities.  Fig.1 Integration of simultaneously assayed modalities using Schema. \n Methods Correlation-based alignment and quadratic programming optimization \nIntuitive notion: pairwise distances in the transformed space can be highly correlated (Pearson correlation coefficient) with pairwise distances under each metric. \nMathematical formulation\n\\(D_j = \\{X_i^j：i=1,2, \\ldots,N;\\ j=1,2,\\ldots,r \\}\\).\n\\(N\\) deotes observations across \\(r\\) datasets \\(Dj\\).\nLet \\(D_1\\) be the primary dataset with \\(k\\) dimension.\n\\(\\rho_j(x_n^j,x_m^j)\\) is the squared distance between observations \\(n\\) and \\(m\\) in \\(D_j\\).\n\\(\\Omega\\) is limited to scaling transform. \\(\\Omega(D) = \\{diag(u)·x,x\\in D \\}\\), for \\(u \\in R^k\\) and \\(diag(u)\\) is a \\(k\\times k\\) diagonal matrix.\nThe squared distance between points under the transformation is given by \\[ \\rho^*(x_n,x_m) = ||diag(u)x_n- diag(u)x_m||^2 = diag(w)||x_n-x_m||^2 \\] where \\(w_i=u_i^2\\). And \\(u\\) acts as a feature-weighting mechanism: i.e., \\(u_i\\) being large means that the \\(i_th\\) coordinate of \\(D_1\\) is important.\nThe alignment between \\(\\rho^*\\) and \\(\\rho_j\\) is given by Pearson correlation. And the goal is to maximize the Corr \\[ Corr(\\rho^*,\\rho_j) = \\frac{Cov(\\rho^*,\\rho_j)}{(Var(\\rho^*)Var(\\rho_j))^{\\frac{1}{2}}} \\] Each modality could be weighted by user. Besides, the correlation allows a hard constraint. And the goal is \\[ \\Bigl\\{ \\sum\\limits_{j=2}^r Corr \\Bigl(\\rho^*(w),\\rho_j \\Bigr) \\Bigr\\} \\\\ Corr \\Bigl(\\rho^*(w),\\rho_1 \\Bigr) \\geqslant s \\] \\(\\rho^*\\) is the function of \\(w\\)\n\nSetting up the quadratic program\nGerenal quadratic programming (QP) is the solution of constrained least squares problem, the gernal form is \\[ \\mathop{min}\\ v^T Qv + q^T v \\\\ Gv \\leq h \\\\ Av=b \\] where \\(Q\\) is a positive semidefinite (psd) matrix.\nLet \\(w_i=u_i^2,w_i \\in R^k, \\delta_{ij}=(x_i-x_j)^2,\\delta_{ij} \\in R^k\\) and \\(P\\) be the set of pairs observations \\(P=\\{i,j\\},1 \\leqslant i \\leqslant j \\leqslant N\\).\nAs \\(Cov(X,Y) = E[X,Y]-E[X]E[Y], Var(X)=E[X^2]-W[X]^2\\) \\[ \\begin{aligned} Cov\\left(w,\\rho_l\\right) \u0026=\\frac{1}{\\left|P\\right|}\\sum_{\\left\\{i,j\\right\\}\\in P}\\rho_l\\left(x_i^l, x_j^l\\right)\\delta_{ij}^Tw-\\frac{1}{\\left|P\\right|^2}\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^Tw \\sum_{\\left\\{i,j\\right\\}\\in P\\ }\\rho_l\\left(x_i^l, x_j^l\\right) \\\\ \u0026= \\left(\\frac{1}{\\left|P\\right|}a_l-\\frac{1}{\\left|P\\right|^2}b_l\\right)^Tw\\ \\end{aligned} \\]\n\\[ \\begin{aligned} Var\\left(w\\right)\u0026=\\frac{1}{\\left|P\\right|}\\sum_{\\left\\{i,j\\right\\}\\in P}w^T\\delta_{ij}\\delta_{ij}^Tw-\\frac{1}{\\left|P\\right|^2}\\left(\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^Tw\\right)^2\\ \\\\ \u0026=w^T\\left(\\frac{1}{\\left|P\\right|}S-\\frac{1}{\\left|P\\right|^2}T\\right)w \\end{aligned} \\] where \\(a_l\\) and \\(b_l\\) are \\(k\\)-dimensional vectors depend only on \\(D_l\\),\\(T\\) and \\(T\\) are \\(N \\times k\\) depent on \\(D_1\\), \\[ a_l =\\sum_{\\left\\{i,j\\right\\}\\in P}\\rho_l\\left(x_i^{\\left(l\\right)},x_j^{\\left(l\\right)}\\right)\\delta_{ij} \\] \\[ b_l = (\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^Tw)(\\sum_{\\left\\{i,j\\right\\}\\in P\\ }\\rho_l\\left(x_i^l, x_j^l\\right)) \\] \\[ S= \\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}\\delta_{ij}^T \\] \\[ T=(\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij})(\\sum_{\\left\\{i,j\\right\\}\\in P}\\delta_{ij}^T) \\]\nAs QP cann’t handle quotients or square roots. Maximizing a quotient can be reframed as maximizing the numerator (the covariance), minimizing the denominator (the variance), or both. So the optimization problem is \\[ \\sum_{j=1}^r\\ r_j Cov(w,\\rho_j)-\\alpha Var(w) -\\lambda||w-1||^2 \\\\ subject\\ to:\\ Cov(w,\\rho_1) \\geqslant \\beta \\\\ w \\succcurlyeq 0 \\] The mapping from the general optimization problem to the QP framework is as follows \\[ v =w \\\\ Q = \\frac{1}{|P|}S-\\frac{1}{|P|^2}T+ \\lambda I_k \\\\ q = -2\\lambda0-\\sum_{j=1}^r r_l (\\frac{1}{|P|}a_l- \\frac{1}{|P|^2}b_l) \\]\nUser can define the hyperparameters \\(s\\) and \\(t\\) \\[ Corr(\\rho^*,\\rho_1) \\geqslant s \\\\ \\frac{max\\{w\\}}{\\sum |w_i|} \\leqslant t \\] The choice of \\(\\lambda\\) controls whether \\(w\\) satisfies \\(t\\), and \\(\\alpha ,\\beta\\) controls whether \\(Corr\\) satisfies \\(s\\).\n\n\n参考链接：\n度量学习/对比学习入门: 论文阅读笔记-Deep Metric Learning Neighbourhood Component Analysis Schema: metric learning enables interpretable synthesis of heterogeneous single-cell modalities \n",
  "wordCount" : "827",
  "inLanguage": "en",
  "datePublished": "2022-03-11T00:00:00Z",
  "dateModified": "2022-03-11T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yupines.github.io/post/metric-learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "yupines",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yupines.github.io/icon/favicon.ico"
    }
  }
}
</script><script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script type="text/javascript">
function (selector) {
  return Array.prototype.slice.call(document.querySelectorAll(selector), 0);
}

function replace_code_to_span(node) {
  is_inline_math = /^$(.)$$/.exec(node.textContent);
  is_display_math = /^$$(.)$$$/ms.exec(node.textContent) ||
          /^\begin{.+}(.*)\end{.+}/ms.exec(node.textContent);
  if (is_inline_math || is_display_math) {
    var parent = node.parentNode;
    var replacement = document.createElement('span');
    if (is_display_math) {
      replacement.class = "yuuki_mathjax_inline"
    } else {
      replacement.class = "yuuki_mathjax_display"
    }
    replacement.textContent = node.textContent;
    parent.replaceChild(replacement, node);
  }
}

document.addEventListener('DOMContentLoaded', function() {
  yuuki_query_all_nodes_as_array("code").map(replace_code_to_span);
});
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['\\(','\\)']]},
  displayMath: [['$$','$$'], ['\\[','\\]']]
});
</script>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yupines.github.io/" accesskey="h" title="yupines (Alt + H)">yupines</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yupines.github.io/me/" title="About me">
                    <span>About me</span>
                </a>
            </li>
            <li>
                <a href="https://yupines.github.io/post/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yupines.github.io/post/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://yupines.github.io/post/" title="Series">
                    <span>Series</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Metric Learning
    </h1>
    <div class="post-meta"><span title='2022-03-11 00:00:00 +0000 UTC'>March 11, 2022</span>

</div>
  </header> 
  <div class="post-content"><p><font size=3 >
在机器学习中，高维数据降维的目的主要是找到合适的距离度量。而度量学习的目的则是直接从原数据中学习出一个合适的距离度量。<br>
</font><br> <br><br></p>
<p><font size=5 ><strong> （一）.背景知识 </strong> </font><br><br>
<font size=4 ><strong> 1.马氏距离（Mahalanobis Distance） </strong>
</font><br><br> <font size=3 >
度量学习中常用的距离指标，可以应对高维线性分布的数据中各维度非独立的问题。此类方法一般应用马氏距离，其通过投影矩阵对源向量投影后再进行距离的度量，对两个<span
class="math inline">\(d\)</span>维样本<span
class="math inline">\(x_i,x_j\)</span>之间的马氏距离为<br> <span
class="math display">\[ dist_{mah}(x_i,x_j) = \sqrt{(x_i -
x_j)^T\Sigma^{-1}(x_i - x_j)}
\]</span> <span class="math inline">\(\Sigma\)</span>
为协方差矩阵.<br></p>
<p>从欧式距离出发，<span class="math inline">\(d\)</span>维样本<span
class="math inline">\(x_i,x_j\)</span>之间的欧式距离为 <span
class="math display">\[ dist_{ed}^2(x_i,y_i) = ||x_i-x_j||^2 =
dist_{i,j,1}^2 + dist_{i,j,2}^2 + \ldots + dist_{i,j,d}^2
\]</span> 对每一维<span class="math inline">\(d\)</span>引入权重<span
class="math inline">\(w\)</span>得到 <span
class="math display">\[\begin{aligned}
dist_{wed}^2(x_i,y_i) &amp;= w_1·dist_{i,j,1}^2 + w_2·dist_{i,j,2}^2 +
\ldots + w_d·dist_{i,j,d}^2 \\
&amp;= (x_i-x_j)^TM(x_i-x_j)
\end{aligned}\]</span> 其中<span class="math inline">\(w \geq
0,W=diag(w)，W\)</span>是一个对角矩阵.<br> 将<span
class="math inline">\(W\)</span>换成半正定对称矩阵<span
class="math inline">\(M\)</span>，即得到马氏距离. <span
class="math display">\[ dist_{Mah} = ||x_i-x_j||^2_M
\]</span> <span
class="math inline">\(M\)</span>即是度量矩阵，为使度量矩阵非负，<span
class="math inline">\(M\)</span>必须是（半）正定对称矩阵，满足<span
class="math inline">\(M=PP^T\)</span>.</p>
<p>马氏距离的几何意义:<br />
将变量按照主成分进行旋转，让维度间相互独立，然后进行标准化. <br>
</font><br> <br><br></p>
<p><font size=4 ><strong> 2.近邻分类器（neighbourhood Component
Analysis, NCA） </strong> </font><br><br> <font size=3 > 对<span
class="math inline">\(M\)</span>的学习需要设定目标.
以近邻分类器为例，假设希望提高NCA的性能，则可将<span
class="math inline">\(M\)</span>嵌入到NCA的评价指标.</p>
<p>计算样本<span class="math inline">\(i\)</span>的近邻分布 <span
class="math display">\[ p_{ij} =
\frac{exp(-||x_i-x_j||^2_M)}{\sum\limits_{k\neq i}exp(-||x_i-x_k||^2_M)}
\]</span> <span class="math inline">\(x_j\)</span>对<span
class="math inline">\(x_i\)</span>的影响随着距离的增大而减少.
以留一法(LOO)正确率最大化为目标，计算<span
class="math inline">\(x_i\)</span>的留一法正确率 <span
class="math display">\[ p_i = \sum\limits_{j \in \Omega_i} p_{ij}
\]</span> <span class="math inline">\(\Omega_i\)</span>表示与<span
class="math inline">\(x_i\)</span>同类别的样本的下标集合，整个样本集的留一法正确率为
<span class="math display">\[\sum\limits_{i=1}^m p_i=\sum\limits_{i=1}^m
\sum\limits_{j \in \Omega_i} p_{ij}
\]</span> 将<span class="math inline">\(p_{ij}\)</span>及<span
class="math inline">\(M=PP^T\)</span>代入上式可得NCA的优化目标为 <span
class="math display">\[ \mathop{min}\limits_{P}\quad 1-
\sum\limits_{i=1}^m \sum\limits_{j \in \Omega_i} \frac{exp(-||P^T
x_i-P^Tx_j||^2)}{\sum\limits_{k\neq i}exp(-||P^Tx_i-P^Tx_k||^2)}
\]</span> 记<span class="math inline">\(g_{ij}=exp(-||P^T
x_i-P^Tx_j||^2)\)</span>，则有 <span
class="math display">\[p_{ij}=\frac{g_{ij}}{\sum\limits_{k\neq
i}g_{ik}}\]</span> 由<span class="math inline">\(\frac{\partial
x^TD^TDx}{\partial D}=2Dxx^T\)</span>得到 <span class="math display">\[
\frac{\partial g_{ij}}{\partial A} = -2g_{ij}A(x_i-x_j)(x_i-x_j)^T
\]</span> <span class="math display">\[\begin{aligned}
\frac{\partial p_{ij}}{\partial A} &amp;=
\frac{1}{(\sum\limits_{k\neq i}g_{ik})^2}(\frac{\partial
g_{ij}}{\partial A}\sum\limits_{k \neq i} g_{ik} -g_{ij}\sum\limits_{k
\neq i} \frac{\partial g_{ik}}{\partial A}) \\
&amp;= -2p_{ij}A \Bigl( (x_i-x_j)(x_i-x_j)^Tp_{ij}A \Bigl(
(x_i-x_j)(x_i-x_j)^T- \sum\limits_{k \neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T
\Bigr)
\end{aligned}\]</span> 目标梯度为 <span
class="math display">\[\begin{aligned} \frac{\partial f}{\partial A}
&amp;= \sum\limits_{i=1}^n \sum\limits_{j \in \Omega_i} \frac{\partial
p_{ij}}{\partial A}\\
&amp;= -2A\sum\limits_{i=1}^n \sum\limits_{j \in \Omega_i} p_{ij} \Bigl(
(x_i-x_j)(x_i-x_j)^T- \sum\limits_{k \neq i}p_{ik}(x_i-x_j)(x_i-x_j)^T
\Bigr) \\
&amp;= -2A\sum\limits_{i=1}^n \Bigl(p_i \sum\limits_{k \neq
i}p_{ik}(x_i-x_j)(x_i-x_j)^T- \sum\limits_{j \in \Omega_i}
p_{ij}(x_i-x_j)(x_i-x_j)^T \Bigr) \\
&amp;= 2\sum\limits_{i=1}^n \sum\limits_{j \in
\Omega_i}(p_ip_{ij}-pmask_{ij})(Ax_i-Ax_j)(x_i-x_j)^T
\end{aligned}\]</span> 其中，<span
class="math inline">\(pmask_{ij}=p_{ij},if\ j\in \Omega_i,else\
0\)</span>. 记<span
class="math inline">\(W_{ij}=p_ip_{ij}-pmask_{ij}\)</span>，则有（简略推导）
<span class="math display">\[ \frac{\partial f}{\partial A} =
2(XA^T)^T(diag(sum(W,axis=0))-W-W^T)X
\]</span></p>
<p><br></p>
<p>若已知某些样本相似和不相似，可定义“必连”约束集合 <span
class="math inline">\(\mathcal M\)</span> 和 “勿连”约束集合 <span
class="math inline">\(\mathcal C\)</span>. <span class="math display">\[
\mathop{min}\limits_{M}\quad \sum\limits_{(x_i,x_j) \in \mathcal M}
||x_i - x_j||^2_M \\
s.t.\  \sum\limits_{(x_i,x_k) \in \mathcal C} ||x_i-x_k|| \geqslant 1\\
M \succeq 0
\]</span></p>
<p>其中，<span class="math inline">\(M \succeq 0\)</span> 表明 <span
class="math inline">\(M\)</span>
必须是半正定的，在不相似样本间距大于等于<span
class="math inline">\(1\)</span>的前提下使得相似样本的间距尽可能小。
<br> </font><br> <br><br></p>
<p><font size=5 ><strong> （二）.文献分享 </strong> </font><br><br>
<font size=4 ><strong> Schema: metric learning enables interpretable
synthesis of heterogeneous single-cell modalities </strong>
</font><br><br> <font size=3 > The author uses a principled metric
learning strategy that identifies informative features in a modality to
synthesize disparate modalities into a single coherent
interpretation.</p>
Schema is designed for assays where multiple modalities are
simultaneously measured for each cell. The researcher designates one
highconfidence modality as the primary (i.e., reference) and one or more
of the remaining modalities as secondary. Schema joints multimodality’s
shared cells into single points in a space with associated distance
metric that encapsulates modality-specific similarity between
observations. Then it transforms the primary modality by scaling the
weighted dimensions in order to have a desired correlation with
remaining modalities. The new point locations represent information
synthesized from multiple modalities.
<div data-align="center">
<img src="img/metric_learning_1.png" width="90%" height="120%">
</div>
<p style="text-align: center; font-size:0.75em;">
Fig.1 Integration of simultaneously assayed modalities using Schema.
</p>
<p><br> <strong> Methods <br></p>
<p>Correlation-based alignment and quadratic programming optimization
</strong></p>
<p>Intuitive notion: pairwise distances in the transformed space can be
highly correlated (Pearson correlation coefficient) with pairwise
distances under each metric. <br></p>
<p><br><strong>Mathematical formulation</strong></p>
<p><span class="math inline">\(D_j = \{X_i^j：i=1,2, \ldots,N;\
j=1,2,\ldots,r \}\)</span>.<br />
<span class="math inline">\(N\)</span> deotes observations across <span
class="math inline">\(r\)</span> datasets <span
class="math inline">\(Dj\)</span>.<br />
Let <span class="math inline">\(D_1\)</span> be the primary dataset with
<span class="math inline">\(k\)</span> dimension.</p>
<p><span class="math inline">\(\rho_j(x_n^j,x_m^j)\)</span> is the
squared distance between observations <span
class="math inline">\(n\)</span> and <span
class="math inline">\(m\)</span> in <span
class="math inline">\(D_j\)</span>.</p>
<p><span class="math inline">\(\Omega\)</span> is limited to scaling
transform. <span class="math inline">\(\Omega(D) = \{diag(u)·x,x\in D
\}\)</span>, for <span class="math inline">\(u \in R^k\)</span> and
<span class="math inline">\(diag(u)\)</span> is a <span
class="math inline">\(k\times k\)</span> diagonal matrix.</p>
<p>The squared distance between points under the transformation is given
by <span class="math display">\[ \rho^*(x_n,x_m) = ||diag(u)x_n-
diag(u)x_m||^2 = diag(w)||x_n-x_m||^2
\]</span> where <span class="math inline">\(w_i=u_i^2\)</span>. And
<span class="math inline">\(u\)</span> acts as a feature-weighting
mechanism: i.e., <span class="math inline">\(u_i\)</span> being large
means that the <span class="math inline">\(i_th\)</span> coordinate of
<span class="math inline">\(D_1\)</span> is important.</p>
<p>The alignment between <span class="math inline">\(\rho^*\)</span> and
<span class="math inline">\(\rho_j\)</span> is given by Pearson
correlation. And the goal is to maximize the Corr <span
class="math display">\[ Corr(\rho^*,\rho_j) =
\frac{Cov(\rho^*,\rho_j)}{(Var(\rho^*)Var(\rho_j))^{\frac{1}{2}}}
\]</span> Each modality could be weighted by user. Besides, the
correlation allows a hard constraint. And the goal is <span
class="math display">\[ \Bigl\{ \sum\limits_{j=2}^r Corr
\Bigl(\rho^*(w),\rho_j  \Bigr)   \Bigr\} \\
Corr \Bigl(\rho^*(w),\rho_1 \Bigr)   \geqslant s
\]</span> <span class="math inline">\(\rho^*\)</span> is the function of
<span class="math inline">\(w\)</span></p>
<p><br><strong>Setting up the quadratic program</strong></p>
<p>Gerenal quadratic programming (QP) is the solution of constrained
least squares problem, the gernal form is <span class="math display">\[
\mathop{min}\  v^T Qv + q^T v \\
Gv \leq h \\
Av=b
\]</span> where <span class="math inline">\(Q\)</span> is a positive
semidefinite (psd) matrix.</p>
<p>Let <span class="math inline">\(w_i=u_i^2,w_i \in R^k,
\delta_{ij}=(x_i-x_j)^2,\delta_{ij} \in R^k\)</span> and <span
class="math inline">\(P\)</span> be the set of pairs observations <span
class="math inline">\(P=\{i,j\},1 \leqslant i \leqslant j \leqslant
N\)</span>.</p>
<p>As <span class="math inline">\(Cov(X,Y) = E[X,Y]-E[X]E[Y],
Var(X)=E[X^2]-W[X]^2\)</span> <span class="math display">\[
\begin{aligned}
Cov\left(w,\rho_l\right)
&amp;=\frac{1}{\left|P\right|}\sum_{\left\{i,j\right\}\in
P}\rho_l\left(x_i^l,
x_j^l\right)\delta_{ij}^Tw-\frac{1}{\left|P\right|^2}\sum_{\left\{i,j\right\}\in
P}\delta_{ij}^Tw \sum_{\left\{i,j\right\}\in P\ }\rho_l\left(x_i^l,
x_j^l\right) \\
&amp;=
\left(\frac{1}{\left|P\right|}a_l-\frac{1}{\left|P\right|^2}b_l\right)^Tw\
\end{aligned} \]</span></p>
<p><span class="math display">\[ \begin{aligned}
Var\left(w\right)&amp;=\frac{1}{\left|P\right|}\sum_{\left\{i,j\right\}\in
P}w^T\delta_{ij}\delta_{ij}^Tw-\frac{1}{\left|P\right|^2}\left(\sum_{\left\{i,j\right\}\in
P}\delta_{ij}^Tw\right)^2\ \\
&amp;=w^T\left(\frac{1}{\left|P\right|}S-\frac{1}{\left|P\right|^2}T\right)w
\end{aligned} \]</span> where <span class="math inline">\(a_l\)</span>
and <span class="math inline">\(b_l\)</span> are <span
class="math inline">\(k\)</span>-dimensional vectors depend only on
<span class="math inline">\(D_l\)</span>,<span
class="math inline">\(T\)</span> and <span
class="math inline">\(T\)</span> are <span class="math inline">\(N
\times k\)</span> depent on <span class="math inline">\(D_1\)</span>,
<span class="math display">\[ a_l =\sum_{\left\{i,j\right\}\in
P}\rho_l\left(x_i^{\left(l\right)},x_j^{\left(l\right)}\right)\delta_{ij}
\]</span> <span class="math display">\[ b_l =
(\sum_{\left\{i,j\right\}\in
P}\delta_{ij}^Tw)(\sum_{\left\{i,j\right\}\in P\ }\rho_l\left(x_i^l,
x_j^l\right))
\]</span> <span class="math display">\[ S= \sum_{\left\{i,j\right\}\in
P}\delta_{ij}\delta_{ij}^T
\]</span> <span class="math display">\[ T=(\sum_{\left\{i,j\right\}\in
P}\delta_{ij})(\sum_{\left\{i,j\right\}\in P}\delta_{ij}^T)
\]</span></p>
<p>As QP cann’t handle quotients or square roots. Maximizing a quotient
can be reframed as maximizing the numerator (the covariance), minimizing
the denominator (the variance), or both. So the optimization problem is
<span class="math display">\[ \sum_{j=1}^r\  r_j Cov(w,\rho_j)-\alpha
Var(w) -\lambda||w-1||^2 \\
subject\ to:\ Cov(w,\rho_1) \geqslant \beta \\
w \succcurlyeq 0
\]</span> The mapping from the general optimization problem to the QP
framework is as follows <span class="math display">\[ v =w \\
Q = \frac{1}{|P|}S-\frac{1}{|P|^2}T+ \lambda I_k \\
q = -2\lambda0-\sum_{j=1}^r r_l (\frac{1}{|P|}a_l- \frac{1}{|P|^2}b_l)
\]</span></p>
<p>User can define the hyperparameters <span
class="math inline">\(s\)</span> and <span
class="math inline">\(t\)</span> <span class="math display">\[
Corr(\rho^*,\rho_1) \geqslant s \\
\frac{max\{w\}}{\sum |w_i|} \leqslant t
\]</span> The choice of <span class="math inline">\(\lambda\)</span>
controls whether <span class="math inline">\(w\)</span> satisfies <span
class="math inline">\(t\)</span>, and <span class="math inline">\(\alpha
,\beta\)</span> controls whether <span
class="math inline">\(Corr\)</span> satisfies <span
class="math inline">\(s\)</span>.</p>
<p><br> </font><br> <br><br></p>
<p><font size=2 > <strong>参考链接：</strong><br>
<a href="https://zhuanlan.zhihu.com/p/348998459"> 度量学习/对比学习入门:
论文阅读笔记-Deep Metric Learning <br>
<a href="https://zhuanlan.zhihu.com/p/48371593"> Neighbourhood Component
Analysis <br>
<a href="https://genomebiology.biomedcentral.com/track/pdf/10.1186/s13059-021-02313-2.pdf">
Schema: metric learning enables interpretable synthesis of heterogeneous
single-cell modalities <br> </font></p>


  </div>

  <footer class="post-footer">
<nav class="paginav">
  <a class="prev" href="https://yupines.github.io/post/manifold-learning/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Manifold Learning</span>
  </a>
  <a class="next" href="https://yupines.github.io/post/pca-svd/">
    <span class="title">Next Page »</span>
    <br>
    <span>Principle Component Analysis</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Metric Learning on twitter"
        href="https://twitter.com/intent/tweet/?text=Metric%20Learning&amp;url=https%3a%2f%2fyupines.github.io%2fpost%2fmetric-learning%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Metric Learning on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyupines.github.io%2fpost%2fmetric-learning%2f&amp;title=Metric%20Learning&amp;summary=Metric%20Learning&amp;source=https%3a%2f%2fyupines.github.io%2fpost%2fmetric-learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Metric Learning on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fyupines.github.io%2fpost%2fmetric-learning%2f&title=Metric%20Learning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Metric Learning on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyupines.github.io%2fpost%2fmetric-learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Metric Learning on whatsapp"
        href="https://api.whatsapp.com/send?text=Metric%20Learning%20-%20https%3a%2f%2fyupines.github.io%2fpost%2fmetric-learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Metric Learning on telegram"
        href="https://telegram.me/share/url?text=Metric%20Learning&amp;url=https%3a%2f%2fyupines.github.io%2fpost%2fmetric-learning%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://yupines.github.io/">yupines</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
